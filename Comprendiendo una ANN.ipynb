{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Plasmando en c칩digo el funcionamiento de las redes neuronales Artificiales - Parte 1</h1>\n",
    "\n",
    "### Para que podamos entrar en contexto sobre nuestro tema principal, primero les quiero hacer hacer una peque침a explicaci칩n de c칩mo funciona una neurona simple en el ser humano, cuyo compratmiento es el que queremos imitar en una m치quina.\n",
    "\n",
    "\n",
    "En este m칩dulo aprenderemos los conecptos b치sicos y las noticiones aritm칠ticas y estadistas de c칩mo funciona una red neuronal artificial as칤 como entener con un c칩digo simple su funcionamiento y luego utilizar librer칤as conocidas como TensorFlow 2.0\n",
    "\n",
    "## 1. La Neurona Humana\n",
    "![NeuronUrl](https://media.giphy.com/media/5h9EHCvA0OR2/giphy.gif \"neuron\")\n",
    "\n",
    "En el gif podemos apreciar las redes nueronales y lo incre칤ble de esto es poder entender c칩mo imitar el compartamiento de una neurona humana con la de una m치quina 쯏 por qu칠 hacemos esto? es simple, nuestro cerebro humano es una m치quina compleja capaz de resolver operaciones -que para una computadora parece muy compleja- con much칤sima facilidad. Imaginate poder recrear, la informaci칩n que recibimos con nuestros ojos desde una c치mara, la percepci칩n de colores, procesarlos y obtener informaci칩n para que realice una tarea que nos ayude. Imitar el funcionamiento del cerebro humano nos ayuda a extender nuestras posibilidades como seres humanos.\n",
    "\n",
    "Entonces nuestro desaf칤o principal est치 en como recrear una neurona. Para ello, vamos a ver analizar su funcionamiento.\n",
    "\n",
    "![RamonUrl](https://supercurioso.com/wp-content/uploads/2014/12/ramonycajalelartistadelaneurona.jpg \"ramon\")\n",
    "\n",
    "En el a침o 1899 Santiago Ram칩n y Cajal (imagen derecha) lo que hizo fue te침ir las neuronas y observarlas bajo el microscopio y traz칩 la imagen de lo que el cre칤a se trataba de una neurona individual (imagen izquierda).\n",
    "\n",
    "A침os m치s tarde, con la llegada de nuevas tecnolog칤as permitieron una mejor observaci칩n y como mostraremos en la imagen m치s abajo, una estructura real y gr치fica de la neurona es muy parecida a la imagen de Santiago.\n",
    "\n",
    "![EstructuraUrl](https://respuestas.tips/wp-content/uploads/2013/10/partes-de-una-neurona.jpg \"estructura\")\n",
    "\n",
    "En realidad, es mucho m치s compleja en cuanto a todas las partes que la componen, pero quiero que nos enfoquemos en tres partes importantes de la neurona, las dendritas, el ax칩n y el n칰cleo.\n",
    "\n",
    "Lo que tenemos que entender aqu칤 es que las neuronas por s칤 solas no sirven, deben convivir con otras para su correcto funcionamiento para lograr cosas incre칤bles que ya conocemos.\n",
    "\n",
    "쮺칩mo trabajan juntas?\n",
    "\n",
    "Bueno, las dendritas, reciben la informaci칩n, pasa por el n칰cleo y el ax칩n se encarga de llevar esta informaci칩n a otra neurona, b치sicamente transmiten impulsos el칠ctricos desde el ax칩n a las dendritas de otra neurona y este proceso es al que se le conoce como sinapsis y para que se hagan una idea nuestra cerebro en edad adulta cuenta con 86 mil millones de neuronas en promedio.\n",
    "\n",
    "Ahora, vamos a empezar a estudiar como logramos plasmar la neurona humana en una m치quina.\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/642/1*ifwiDM2fRKze9VBG2p6_Bg.png \"estructura\")\n",
    "\n",
    "Entre las d칠cadas del 1950 y 1960 el psicol칩go y cient칤fico estadounidense, inspirado en el trabajo de otros cient칤ficos, crea algo que hoy conocemos como perceptr칩n (imagen de arriba). Que es la 칰nidad b치sica de inferencia o una neurona artificial.\n",
    "\n",
    "Ahora imaginemos a las fechas como dendritas y las X como informaci칩n que le pasa otra neurona, entonces las flechas ser칤an las sinapsis. En este caso la se침al que le pasamos, vendr칤a a ser una valor de entrada y a partir de estos valores generan un valor de salida. A las entradas les llamamos variables independientes porque pueden ser cualquier valor por separado y a la salida como variable dependiente porque es un resultado que depende de las entradas. Y a las la sinapsis que vemos ahora los llamaremos pesos \"w\" como la imagen de abajo:\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/642/1*ifwiDM2fRKze9VBG2p6_Bg.png \"estructura\")\n",
    "\n",
    "Esta sinapsis que ahora le llamaremos pesos o com칰nmente representado por \"w\" son fundamentales para el correcto funcionamiento de nuestro perceptr칩n porque estos valores de pesos se utilizan para que pueda aprender qu칠 valor de entrada a la neuora es importante y que no. De hecho lo 칰nico que ajustamos cuando entrenamos una red neuronal es ajustar los valores de los pesos constantemente, pero 쯣or qu칠?\n",
    "\n",
    "El entrenamiento tiene dos pasos\n",
    "\n",
    "    Primer paso:\n",
    "    El entrenamiento lo que hace es una suma total del valor de entrada multiplicado por los pesos es decir: x1.w1 + x2.w2 + xn*wn. A esto se le conoce como suma ponderada y podemos representarlo con la siguiente ecuaci칩n:\n",
    "    \n",
    "   $$\\sum\\limits_{i=0}^{m} {x_i*w_i}$$  \n",
    "    \n",
    "    Segundo paso:\n",
    "    A nuestra suma ponderada le aplicamos una funci칩n de activaci칩n delta (no se preocupen veremos a continuaci칩n que es esta funci칩n). Quedando as칤 finalmente \n",
    "    \n",
    "   $$\\delta(\\sum\\limits_{i=0}^{m} {x_i*w_i})$$ \n",
    "\n",
    "\n",
    "## 2. Funci칩n de activaci칩n\n",
    "\n",
    "La funci칩n de activaci칩n es la que me permite definir el comportamiento de mi neurona y de cada capa, esta funci칩n decide que valor pasa o se queda. Hablaremos brevemente de 4 de ellas.\n",
    "\n",
    "- Threshold Function o escal칩n unitario\n",
    "![EstructuraUrl](https://miro.medium.com/max/960/0*etGrZj_m0spvgN9n.png \"estructura\")\n",
    "Esta es la funci칩n m치s simple, como ver치n cuando recibe valore negativos (menores a 0) la funci칩n est치 en cero y cuando recibe valores positivos (mayores a 0) la funci칩n pasa a ser 1, es decir nuestro valor de salida siempre va a ser 1 y 0 nada m치s. Ahora que comprendemos como es una funci칩n de activaci칩n b치sica vamos a uno un poco m치s compleja.\n",
    "\n",
    "\n",
    "- Funci칩n Sigmoide\n",
    "![EstructuraUrl](https://miro.medium.com/max/744/1*zSDNMn4z_hmYing0wDCSoQ.png \"sigmoide\")\n",
    "Como vemos esta funci칩n es un poco m치s suave, pasa por todos los valores entre 0 y 1, es muy 칰ltil cuando queremos calcular probabilidades en nuestra salida.\n",
    "\n",
    "\n",
    "- Funci칩n Rectificadora\n",
    "![EstructuraUrl](https://miro.medium.com/max/736/1*RthaUGkni6YCtwlAKsadsg.png \"recti\")\n",
    "Es una de las funciones m치s utilizadas en Machine Learning, esto lo que hace simplemente es entregarnos los valores cuando se치n mayores a cero, es decir, valor positivo. Para cada valor positivo un valor de la curva.\n",
    "\n",
    "\n",
    "- Tangente Hiperb칩lica\n",
    "![EstructuraUrl](https://miro.medium.com/max/772/1*sYnLa9djWZ7BozqUV5lNmw.png \"hiper\")\n",
    "Esta funci칩n es muy similar a la Sigmoide pero va desde -1 a 1 los valores.\n",
    "\n",
    "Quisiera que puedas tener en mente esto ya que todav칤a no entraremos a profundidad pero si los veremos m치s adelante.\n",
    "\n",
    "## 3. 쮺칩mo funcionan las Redes Neuronales Artificiales en un caso real?\n",
    "\n",
    "Vamos a imaginar que hacemos una clasificaci칩n, vamos a usar un dataset, en este caso y para ejemplo el de Titanic, donde nos trae una lista de pasajeros y si sobrevivieron o no. A continuaci칩n vamos a representar nuestro perceptr칩n con las variables y se ve as칤:\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/1084/1*_pD95maLHcEL8Hc39EMJGA.png \"ti\")\n",
    "\n",
    "Para este caso es s칰per f치cil calcular ya que el resultado es simplemente la suma ponderada con la funci칩n de activaci칩n. Pero que pasa si a침ado m치s capacas de a nuestro modelo, es decir, todas las entradas iran conectadas a la capa siguiente como vemos en la siguiente imagen, eso significa que podr칤amos predecir m치s valores, aunque no piensen que si tenemos m치s capa mejora el resultado.\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/1084/1*YjtSTddpDPCscJSXcQblBg.png \"j\")\n",
    "\n",
    "Mientras m치s capas tengas m치s nivel de abstracci칩n va a tener nuestra red. Recuerden que b치sicamente cada una recibe un valor de peso distinto por lo que puede o no recibir esa entrada x en algunos perceptrones, de esta manera, cada uno realiza una funci칩n difrente, aunque no siempre sea as칤.\n",
    "\n",
    "## 4. 쮺칩mo aprenden las redes neuronales?\n",
    "\n",
    "En programaci칩n hay muchas maneras de llegar a una soluci칩n, por ejemplo, podr칤amos programar todos los casos y respuestas para una soluci칩n posible. Cuando esta tarea lleva variables que no siempre se van a comportar como el algoritmo espera, por ejemplo. Queremos crear algo que reconzca un perro en cualquier fotos in importar la raza. Suena sencillo, clasifico las imagenes, digo que son perros, y listo. Pero esta tarea se complica ya que existen n razas de perros, pueden salir en n tipo de ambientes en la fotograf칤a y con diferentes tipos de poses, para esto, es mucho m치s complicado tener un c칩digo que lo resuleva o programar cada caso, para ello se crean las redes neuronales artificiales, claro, para nosotros es natural saber que hay un perro en la foto pero para la m치quina no.\n",
    "\n",
    "La ventaja de las redes neuronales es que puede \"entender\" los datos que le damos de entrada y hacer una funci칩n para cualquier entrada que le presentemos, y como buen ejemplo hemos dado la imagen del perro.\n",
    "\n",
    "Veamos de nuevo la red neuronal anterior...\n",
    "![EstructuraUrl](https://miro.medium.com/max/1084/1*YjtSTddpDPCscJSXcQblBg.png \"j\")\n",
    "Lo que vamos a hacer es calcular el valor de la salida \"y\" (si muri칩 o no) en base a nuestro dato de entrada \"x\" (variables que vemos arriba). Adem치s de esto le vamos a pasar a Y valores reales de que si muri칩 o no. 쯇ero entonces de que me sirve la predicci칩n en \"y\" si le paso los valores reales?. Es simple, al final de cada paso por la neurona, la m치quina lo que va a a hacer es comparar el valor de salida de la predicci칩n con el valor real, esto se hace a trav칠s de la funci칩n de p칠rdida y podemos verlo en la siguiente ecuaci칩n:\n",
    "$$L = {1 \\over 2}(\\Upsilon - y)^2$$\n",
    "Aqu칤 Upsilon (la y medio rara que se ve ah칤 jajaja) es nuestro valor predecido y la y normal es nuestro valor real. Esta es la m치s com칰n entre las funciones de p칠rdida y esta se llama \"Mean Squered Error\" o la medidad de los errores al cuadrado. Esta funci칩n lo que nos dice es cu치nto es el error que va a tener nuestra red y lo que tenemos que hacer es minimizarla.\n",
    "Esta funci칩n va para atr치s y corrije los pesos para ir minizando los pesos \"w\".\n",
    "\n",
    "## 5. 쮺칩mo es que minimizo mi funci칩n de perdida?\n",
    "\n",
    "Bueno para poder minizar la funci칩n de perdida podr칤amos utilizar algo que se llama el descenso de gradiente (aclaro que este texto no muestra la unica forma pero si trato de simplificar lo m치ximo posible para adentrarnos en este complejo mundo).\n",
    "Lo que hace esta funci칩n es tratar de encontrar los valores de los pesos de modo a que el error se ajuste con la pendiente de la recta dentro del conjunto de valores (puntos azules) que ser칤an las entradas como se aprecia en el siguiente gif:\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/proxy/1*KQVi812_aERFRolz_5G3rA.gif)\n",
    "\n",
    "Pero que pasa si nuestra funci칩n no representa una parabola y tiene varios m칤nimos como se ve a continuaci칩n\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/1400/0*XKMTnu8MwD2wdso6.png)\n",
    "\n",
    "Si nuestro algoritmo solamente encuentra un m칤nimo local no encontrar치 un m칤nimo global que es el que neceitamos para que se ajusten los valores de los pesos.\n",
    "\n",
    "Entonces con el descendo de gradiente lo que nuestra funci칩n hace en cada paso es\n",
    "- Paso el dato 1\n",
    "- Paso el dato 2\n",
    "- Paso el dato 3\n",
    "- Ajusto los pesos\n",
    "\n",
    "Para solucionar el problema anterior donde tenemos varios m칤nimos posibles se utiliza otro algoritmo que hace algo que se llama descenso de gradiente estoc치stico y hace lo siguiente\n",
    "- Paso el dato 1\n",
    "- Ajusto los pesos\n",
    "- Paso el dato 2\n",
    "- Ajusto los pesos\n",
    "- Paso el dato 3\n",
    "- Ajusto los pesos\n",
    "\n",
    "Entonces cada vez que pase por el dato va a justar los pesos y se acercar치 m치s al m칤nimo global ya que se mueve constantemente ante cada paso de datos.\n",
    "\n",
    "## 5. Backpropagation\n",
    "\n",
    "Todo lo que vimos hasta ahora es lo que se llama comunmente Forward Propagation, donde el valor de entrada, pasa por toda la red, pasa por la funci칩n de activaci칩n, se calcula la funci칩n de p칠rdida, luego de este proceso la p칠rdida se debe \"propagar hacia atr치s\" ajustando los valores de los pesos, esto es lo que hace el backpropagation, ir para atr치s y ajustar los valores de los pesos a la vez en simult치neo, es un algoritmo con unas matem치ticas muy avanzadas e interesantes que los puedes mirar por tu cuenta si quieres :)\n",
    "\n",
    "## 6. Manos a la obra\n",
    "\n",
    "Ahora lo que haremos es aplicar una peque침a red neuronal sin utilizar ninguna librer칤a especial excepto la de numpy. Numpy es una liber칤a de python que nos permite realizar operaciones matem치ticas complejas con una f치cil implementaci칩n del c칩digo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Como primer paso lo que haremos es instalar esta liber칤a en caso de que nuestra PC no lo tenga, si est치s usando alg칰n otro editor puedes instalarlo desde tu terminal quitando el signo de exclamaci칩n que tiene en frente.\n",
    "\n",
    "Recuerden que para ejecutar un bloque de c칩digo como el que ven abajo solo deben ubicar el cursos sobre el bloque y presionar SHIFT+ENTER y as칤 sucesivamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.18.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Importamos la librer칤a de numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lo que hacemos en este punto crear una funci칩n que calcule nuestra funci칩n de activaci칩n, que ser칤a una variante de la funci칩n sigmoide.\n",
    "Tengamos en cuenta que creamos esta funci칩n para usarlo en sus dos formas, si queremos como sus derivadas o no.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlin(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lo que hacemos aqu칤 abajo es crear dos matrices, una matriz \"x\" que ya sabemos que es la entrada \"y\" que son los valores reales de la salida osea los resultados reales.\n",
    "\n",
    "Recuerden que esos datos que presentamos en la entrada son valores cualquiera, solo representan una entrada que queremos que nos brinde una salida predictiva.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,0,1],\n",
    "      [0,1,1],\n",
    "      [1,0,1],\n",
    "      [1,1,1]])\n",
    "y = np.array([[0],\n",
    "      [1],\n",
    "      [1],\n",
    "      [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Aqu칤 lo que haremos es crear valores de pesos de manera randomica, que la computadora me los genere por que es lo que voy a ir corrigiendo con el algoritmo. Los imprimo para que los veamos.\n",
    "Supongamos que vamos a tener dos capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
      " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n",
      " [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n",
      "[[-0.5910955 ]\n",
      " [ 0.75623487]\n",
      " [-0.94522481]\n",
      " [ 0.34093502]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "w1 = 2*np.random.random((3,4))-1\n",
    "w2 = 2*np.random.random((4,1))-1\n",
    "\n",
    "print(w1)\n",
    "print(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bueno aqu칤 empieza lo divertido 游땕\n",
    "Supogamos que L1 y L2 son las capas.\n",
    "Lo que vamos a hacer primero es el #Forward Propagation, en este paso primero guardamos en \"L0\" nuestros valores de \"x\" y luego hacemos un producto punto con los valores de los pesos (suma ponderarda) y lo pasamos por la funci칩n de activaci칩n como se puestra en L1, estos valores se multiplican con los valores anteriores y los pesos de la capa 2.\n",
    "\n",
    "Luego en #C치lculo de erro lo que hacemos es calcular nuestro error de los valores reales con nuestras predicciones para hallar el valor de p칠rdida. Esto hacemos restando el valor de real con el valor que se predijo.\n",
    "\n",
    "Recuerdan que hablamos sobre el algoritmo de descenso de gradiente?. Bueno eso se calcula multiplicando el error actual por la derivada de la funci칩n que creamos con las predicciones actuales (por eso decidimos tener una opci칩n de derivada m치s arriba) Recuerde la derivada es la recta tangente en un punto a la funci칩n.\n",
    "\n",
    "#Backpropagation.. Una vez que tengamos el descenso de gradiente (la pendiente) multimplicamos por la transpuesta de los pesos de la capa siguiente y de este modo propagaremos el error hacia atr치s.\n",
    "\n",
    "Y finalmente #ajustamos el valor de los pesos con la transpuesta de las capas por sus respectivas optimizaciones o direcciones hacia donde actualizar los pesos y esto suman a nuestros pesos actuales modificando los dos a la vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(6000):\n",
    "    #Forward Propagation\n",
    "    L0 = x\n",
    "    L1 = nonlin(np.dot(L0,w1))\n",
    "    L2 = nonlin(np.dot(L1,w2)) \n",
    "    \n",
    "    #C치lculo de error\n",
    "    L2_error = y - L2       \n",
    "    \n",
    "    #Calculamos el descenso de gradiente\n",
    "    L2_delta = L2_error*nonlin(L2,deriv=True)\n",
    "    \n",
    "    #Backpropagation\n",
    "    L1_error = L2_delta.dot(w2.T)\n",
    "    L1_delta = L1_error * nonlin(L1,deriv=True)\n",
    "    \n",
    "    #ajustamos el valor de los pesos\n",
    "    w2 += L1.T.dot(L2_delta)\n",
    "    w1 += L0.T.dot(L1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora analicemos los resultados, vemos como aqu칤 nuestro valores \"y\" que son los reales vs los valores que la m치quina predijo que ser칤an. 쯄uy loco no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores reales\n",
      "\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "\n",
      "Valores de las predicciones\n",
      "\n",
      "[[0.00631758]\n",
      " [0.99192018]\n",
      " [0.99291766]\n",
      " [0.0094266 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Valores reales\\n\")\n",
    "print(y)\n",
    "print(\"\\n\")\n",
    "print(\"Valores de las predicciones\\n\")\n",
    "print(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
