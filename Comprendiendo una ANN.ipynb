{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Plasmando en código el funcionamiento de las Redes Neuronales Artificiales - Parte 1</h1>\n",
    "\n",
    "### Para que podamos entrar en contexto sobre nuestro tema principal, primero les quiero hacer hacer una pequeña explicación de cómo funciona una neurona simple en el ser humano, cuyo compratmiento es el que queremos imitar en una máquina.\n",
    "\n",
    "\n",
    "En este módulo aprenderemos los conecptos básicos y las noticiones aritméticas y estadistas de cómo funciona una red neuronal artificial así como entener con un código simple su funcionamiento y luego utilizar librerías conocidas como TensorFlow 2.0\n",
    "\n",
    "## 1. La Neurona Humana\n",
    "![NeuronUrl](https://media.giphy.com/media/5h9EHCvA0OR2/giphy.gif \"neuron\")\n",
    "\n",
    "En el gif podemos apreciar las redes nueronales y lo increíble de esto es poder entender cómo imitar el compartamiento de una neurona humana con la de una máquina ¿Y por qué hacemos esto? es simple, nuestro cerebro humano es una máquina compleja capaz de resolver operaciones -que para una computadora parece muy compleja- con muchísima facilidad. Imaginate poder recrear, la información que recibimos con nuestros ojos desde una cámara, la percepción de colores, procesarlos y obtener información para que realice una tarea que nos ayude. Imitar el funcionamiento del cerebro humano nos ayuda a extender nuestras posibilidades como seres humanos.\n",
    "\n",
    "Entonces nuestro desafío principal está en como recrear una neurona. Para ello, vamos a ver analizar su funcionamiento.\n",
    "\n",
    "![RamonUrl](https://supercurioso.com/wp-content/uploads/2014/12/ramonycajalelartistadelaneurona.jpg \"ramon\")\n",
    "\n",
    "En el año 1899 Santiago Ramón y Cajal (imagen derecha) lo que hizo fue teñir las neuronas y observarlas bajo el microscopio y trazó la imagen de lo que el creía se trataba de una neurona individual (imagen izquierda).\n",
    "\n",
    "Años más tarde, con la llegada de nuevas tecnologías permitieron una mejor observación y como mostraremos en la imagen más abajo, una estructura real y gráfica de la neurona es muy parecida a la imagen de Santiago.\n",
    "\n",
    "![EstructuraUrl](https://respuestas.tips/wp-content/uploads/2013/10/partes-de-una-neurona.jpg \"estructura\")\n",
    "\n",
    "En realidad, es mucho más compleja en cuanto a todas las partes que la componen, pero quiero que nos enfoquemos en tres partes importantes de la neurona, las dendritas, el axón y el núcleo.\n",
    "\n",
    "Lo que tenemos que entender aquí es que las neuronas por sí solas no sirven, deben convivir con otras para su correcto funcionamiento para lograr cosas increíbles que ya conocemos.\n",
    "\n",
    "¿Cómo trabajan juntas?\n",
    "\n",
    "Bueno, las dendritas, reciben la información, pasa por el núcleo y el axón se encarga de llevar esta información a otra neurona, básicamente transmiten impulsos eléctricos desde el axón a las dendritas de otra neurona y este proceso es al que se le conoce como sinapsis y para que se hagan una idea nuestra cerebro en edad adulta cuenta con 86 mil millones de neuronas en promedio.\n",
    "\n",
    "Ahora, vamos a empezar a estudiar como logramos plasmar la neurona humana en una máquina.\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/642/1*ifwiDM2fRKze9VBG2p6_Bg.png \"estructura\")\n",
    "\n",
    "Entre las décadas del 1950 y 1960 el psicológo y científico estadounidense, inspirado en el trabajo de otros científicos, crea algo que hoy conocemos como perceptrón (imagen de arriba). Que es la únidad básica de inferencia o una neurona artificial.\n",
    "\n",
    "Ahora imaginemos a las fechas como dendritas y las X como información que le pasa otra neurona, entonces las flechas serían las sinapsis. En este caso la señal que le pasamos, vendría a ser una valor de entrada y a partir de estos valores generan un valor de salida. A las entradas les llamamos variables independientes porque pueden ser cualquier valor por separado y a la salida como variable dependiente porque es un resultado que depende de las entradas. Y a las la sinapsis que vemos ahora los llamaremos pesos \"w\" como la imagen de abajo:\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/642/1*ifwiDM2fRKze9VBG2p6_Bg.png \"estructura\")\n",
    "\n",
    "Esta sinapsis que ahora le llamaremos pesos o comúnmente representado por \"w\" son fundamentales para el correcto funcionamiento de nuestro perceptrón porque estos valores de pesos se utilizan para que pueda aprender qué valor de entrada a la neuora es importante y que no. De hecho lo único que ajustamos cuando entrenamos una red neuronal es ajustar los valores de los pesos constantemente, pero ¿por qué?\n",
    "\n",
    "El entrenamiento tiene dos pasos\n",
    "\n",
    "    Primer paso:\n",
    "    El entrenamiento lo que hace es una suma total del valor de entrada multiplicado por los pesos es decir: x1.w1 + x2.w2 + xn*wn. A esto se le conoce como suma ponderada y podemos representarlo con la siguiente ecuación:\n",
    "    \n",
    "   $$\\sum\\limits_{i=0}^{m} {x_i*w_i}$$  \n",
    "    \n",
    "    Segundo paso:\n",
    "    A nuestra suma ponderada le aplicamos una función de activación delta (no se preocupen veremos a continuación que es esta función). Quedando así finalmente \n",
    "    \n",
    "   $$\\delta(\\sum\\limits_{i=0}^{m} {x_i*w_i})$$ \n",
    "\n",
    "\n",
    "## 2. Función de activación\n",
    "\n",
    "La función de activación es la que me permite definir el comportamiento de mi neurona y de cada capa, esta función decide que valor pasa o se queda. Hablaremos brevemente de 4 de ellas.\n",
    "\n",
    "- Threshold Function o escalón unitario\n",
    "![EstructuraUrl](https://miro.medium.com/max/960/0*etGrZj_m0spvgN9n.png \"estructura\")\n",
    "Esta es la función más simple, como verán cuando recibe valore negativos (menores a 0) la función está en cero y cuando recibe valores positivos (mayores a 0) la función pasa a ser 1, es decir nuestro valor de salida siempre va a ser 1 y 0 nada más. Ahora que comprendemos como es una función de activación básica vamos a uno un poco más compleja.\n",
    "\n",
    "\n",
    "- Función Sigmoide\n",
    "![EstructuraUrl](https://miro.medium.com/max/744/1*zSDNMn4z_hmYing0wDCSoQ.png \"sigmoide\")\n",
    "Como vemos esta función es un poco más suave, pasa por todos los valores entre 0 y 1, es muy últil cuando queremos calcular probabilidades en nuestra salida.\n",
    "\n",
    "\n",
    "- Función Rectificadora\n",
    "![EstructuraUrl](https://miro.medium.com/max/736/1*RthaUGkni6YCtwlAKsadsg.png \"recti\")\n",
    "Es una de las funciones más utilizadas en Machine Learning, esto lo que hace simplemente es entregarnos los valores cuando seán mayores a cero, es decir, valor positivo. Para cada valor positivo un valor de la curva.\n",
    "\n",
    "\n",
    "- Tangente Hiperbólica\n",
    "![EstructuraUrl](https://miro.medium.com/max/772/1*sYnLa9djWZ7BozqUV5lNmw.png \"hiper\")\n",
    "Esta función es muy similar a la Sigmoide pero va desde -1 a 1 los valores.\n",
    "\n",
    "Quisiera que puedas tener en mente esto ya que todavía no entraremos a profundidad pero si los veremos más adelante.\n",
    "\n",
    "## 3. ¿Cómo funcionan las Redes Neuronales Artificiales en un caso real?\n",
    "\n",
    "Vamos a imaginar que hacemos una clasificación, vamos a usar un dataset, en este caso y para ejemplo el de Titanic, donde nos trae una lista de pasajeros y si sobrevivieron o no. A continuación vamos a representar nuestro perceptrón con las variables y se ve así:\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/1084/1*_pD95maLHcEL8Hc39EMJGA.png \"ti\")\n",
    "\n",
    "Para este caso es súper fácil calcular ya que el resultado es simplemente la suma ponderada con la función de activación. Pero que pasa si añado más capacas de a nuestro modelo, es decir, todas las entradas iran conectadas a la capa siguiente como vemos en la siguiente imagen, eso significa que podríamos predecir más valores, aunque no piensen que si tenemos más capa mejora el resultado.\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/1084/1*YjtSTddpDPCscJSXcQblBg.png \"j\")\n",
    "\n",
    "Mientras más capas tengas más nivel de abstracción va a tener nuestra red. Recuerden que básicamente cada una recibe un valor de peso distinto por lo que puede o no recibir esa entrada x en algunos perceptrones, de esta manera, cada uno realiza una función difrente, aunque no siempre sea así.\n",
    "\n",
    "## 4. ¿Cómo aprenden las redes neuronales?\n",
    "\n",
    "En programación hay muchas maneras de llegar a una solución, por ejemplo, podríamos programar todos los casos y respuestas para una solución posible. Cuando esta tarea lleva variables que no siempre se van a comportar como el algoritmo espera, por ejemplo. Queremos crear algo que reconzca un perro en cualquier fotos in importar la raza. Suena sencillo, clasifico las imagenes, digo que son perros, y listo. Pero esta tarea se complica ya que existen n razas de perros, pueden salir en n tipo de ambientes en la fotografía y con diferentes tipos de poses, para esto, es mucho más complicado tener un código que lo resuleva o programar cada caso, para ello se crean las redes neuronales artificiales, claro, para nosotros es natural saber que hay un perro en la foto pero para la máquina no.\n",
    "\n",
    "La ventaja de las redes neuronales es que puede \"entender\" los datos que le damos de entrada y hacer una función para cualquier entrada que le presentemos, y como buen ejemplo hemos dado la imagen del perro.\n",
    "\n",
    "Veamos de nuevo la red neuronal anterior...\n",
    "![EstructuraUrl](https://miro.medium.com/max/1084/1*YjtSTddpDPCscJSXcQblBg.png \"j\")\n",
    "Lo que vamos a hacer es calcular el valor de la salida \"y\" (si murió o no) en base a nuestro dato de entrada \"x\" (variables que vemos arriba). Además de esto le vamos a pasar a Y valores reales de que si murió o no. ¿Pero entonces de que me sirve la predicción en \"y\" si le paso los valores reales?. Es simple, al final de cada paso por la neurona, la máquina lo que va a a hacer es comparar el valor de salida de la predicción con el valor real, esto se hace a través de la función de pérdida y podemos verlo en la siguiente ecuación:\n",
    "$$L = {1 \\over 2}(\\Upsilon - y)^2$$\n",
    "Aquí Upsilon (la y medio rara que se ve ahí jajaja) es nuestro valor predecido y la y normal es nuestro valor real. Esta es la más común entre las funciones de pérdida y esta se llama \"Mean Squered Error\" o la medidad de los errores al cuadrado. Esta función lo que nos dice es cuánto es el error que va a tener nuestra red y lo que tenemos que hacer es minimizarla.\n",
    "Esta función va para atrás y corrije los pesos para ir minizando los pesos \"w\".\n",
    "\n",
    "## 5. ¿Cómo es que minimizo mi función de perdida?\n",
    "\n",
    "Bueno para poder minizar la función de perdida podríamos utilizar algo que se llama el descenso de gradiente (aclaro que este texto no muestra la unica forma pero si trato de simplificar lo máximo posible para adentrarnos en este complejo mundo).\n",
    "Lo que hace esta función es tratar de encontrar los valores de los pesos de modo a que el error se ajuste con la pendiente de la recta dentro del conjunto de valores (puntos azules) que serían las entradas como se aprecia en el siguiente gif:\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/proxy/1*KQVi812_aERFRolz_5G3rA.gif)\n",
    "\n",
    "Pero que pasa si nuestra función no representa una parabola y tiene varios mínimos como se ve a continuación\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/1400/0*XKMTnu8MwD2wdso6.png)\n",
    "\n",
    "Si nuestro algoritmo solamente encuentra un mínimo local no encontrará un mínimo global que es el que neceitamos para que se ajusten los valores de los pesos.\n",
    "\n",
    "Entonces con el descendo de gradiente lo que nuestra función hace en cada paso es\n",
    "- Paso el dato 1\n",
    "- Paso el dato 2\n",
    "- Paso el dato 3\n",
    "- Ajusto los pesos\n",
    "\n",
    "Para solucionar el problema anterior donde tenemos varios mínimos posibles se utiliza otro algoritmo que hace algo que se llama descenso de gradiente estocástico y hace lo siguiente\n",
    "- Paso el dato 1\n",
    "- Ajusto los pesos\n",
    "- Paso el dato 2\n",
    "- Ajusto los pesos\n",
    "- Paso el dato 3\n",
    "- Ajusto los pesos\n",
    "\n",
    "Entonces cada vez que pase por el dato va a justar los pesos y se acercará más al mínimo global ya que se mueve constantemente ante cada paso de datos.\n",
    "\n",
    "## 5. Backpropagation\n",
    "\n",
    "Todo lo que vimos hasta ahora es lo que se llama comunmente Forward Propagation, donde el valor de entrada, pasa por toda la red, pasa por la función de activación, se calcula la función de pérdida, luego de este proceso la pérdida se debe \"propagar hacia atrás\" ajustando los valores de los pesos, esto es lo que hace el backpropagation, ir para atrás y ajustar los valores de los pesos a la vez en simultáneo, es un algoritmo con unas matemáticas muy avanzadas e interesantes que los puedes mirar por tu cuenta si quieres :)\n",
    "\n",
    "## 6. Manos a la obra\n",
    "\n",
    "Ahora lo que haremos es aplicar una pequeña red neuronal sin utilizar ninguna librería especial excepto la de numpy. Numpy es una libería de python que nos permite realizar operaciones matemáticas complejas con una fácil implementación del código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Como primer paso lo que haremos es instalar esta libería en caso de que nuestra PC no lo tenga, si estás usando algún otro editor puedes instalarlo desde tu terminal quitando el signo de exclamación que tiene en frente.\n",
    "\n",
    "Recuerden que para ejecutar un bloque de código como el que ven abajo solo deben ubicar el cursos sobre el bloque y presionar SHIFT+ENTER y así sucesivamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.18.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Importamos la librería de numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lo que hacemos en este punto crear una función que calcule nuestra función de activación, que sería una variante de la función sigmoide.\n",
    "Tengamos en cuenta que creamos esta función para usarlo en sus dos formas, si queremos como sus derivadas o no.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlin(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lo que hacemos aquí abajo es crear dos matrices, una matriz \"x\" que ya sabemos que es la entrada \"y\" que son los valores reales de la salida osea los resultados reales.\n",
    "\n",
    "Recuerden que esos datos que presentamos en la entrada son valores cualquiera, solo representan una entrada que queremos que nos brinde una salida predictiva.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,0,1],\n",
    "      [0,1,1],\n",
    "      [1,0,1],\n",
    "      [1,1,1]])\n",
    "y = np.array([[0],\n",
    "      [1],\n",
    "      [1],\n",
    "      [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Aquí lo que haremos es crear valores de pesos de manera randomica, que la computadora me los genere por que es lo que voy a ir corrigiendo con el algoritmo. Los imprimo para que los veamos.\n",
    "Supongamos que vamos a tener dos capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
      " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n",
      " [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n",
      "[[-0.5910955 ]\n",
      " [ 0.75623487]\n",
      " [-0.94522481]\n",
      " [ 0.34093502]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "w1 = 2*np.random.random((3,4))-1\n",
    "w2 = 2*np.random.random((4,1))-1\n",
    "\n",
    "print(w1)\n",
    "print(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bueno aquí empieza lo divertido 😊\n",
    "Supogamos que L1 y L2 son las capas.\n",
    "Lo que vamos a hacer primero es el #Forward Propagation, en este paso primero guardamos en \"L0\" nuestros valores de \"x\" y luego hacemos un producto punto con los valores de los pesos (suma ponderarda) y lo pasamos por la función de activación como se puestra en L1, estos valores se multiplican con los valores anteriores y los pesos de la capa 2.\n",
    "\n",
    "Luego en #Cálculo de erro lo que hacemos es calcular nuestro error de los valores reales con nuestras predicciones para hallar el valor de pérdida. Esto hacemos restando el valor de real con el valor que se predijo.\n",
    "\n",
    "¿Recuerdan que hablamos sobre el algoritmo de descenso de gradiente?. Bueno eso se calcula multiplicando el error actual por la derivada de la función que creamos con las predicciones actuales (por eso decidimos tener una opción de derivada más arriba) Recuerde la derivada es la recta tangente en un punto a la función.\n",
    "\n",
    "#Backpropagation.. Una vez que tengamos el descenso de gradiente (la pendiente) multimplicamos por la transpuesta de los pesos de la capa siguiente y de este modo propagaremos el error hacia atrás.\n",
    "\n",
    "Y finalmente #ajustamos el valor de los pesos con la transpuesta de las capas por sus respectivas optimizaciones o direcciones hacia donde actualizar los pesos y esto suman a nuestros pesos actuales modificando los dos a la vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(6000):\n",
    "    #Forward Propagation\n",
    "    L0 = x\n",
    "    L1 = nonlin(np.dot(L0,w1))\n",
    "    L2 = nonlin(np.dot(L1,w2)) \n",
    "    \n",
    "    #Cálculo de error\n",
    "    L2_error = y - L2       \n",
    "    \n",
    "    #Calculamos el descenso de gradiente\n",
    "    L2_delta = L2_error*nonlin(L2,deriv=True)\n",
    "    \n",
    "    #Backpropagation\n",
    "    L1_error = L2_delta.dot(w2.T)\n",
    "    L1_delta = L1_error * nonlin(L1,deriv=True)\n",
    "    \n",
    "    #ajustamos el valor de los pesos\n",
    "    w2 += L1.T.dot(L2_delta)\n",
    "    w1 += L0.T.dot(L1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora analicemos los resultados, vemos como aquí nuestro valores \"y\" que son los reales vs los valores que la máquina predijo que serían. ¿Muy loco no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores reales\n",
      "\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "\n",
      "Valores de las predicciones\n",
      "\n",
      "[[0.00631758]\n",
      " [0.99192018]\n",
      " [0.99291766]\n",
      " [0.0094266 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Valores reales\\n\")\n",
    "print(y)\n",
    "print(\"\\n\")\n",
    "print(\"Valores de las predicciones\\n\")\n",
    "print(L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Ahora sí, el dataset de Titanic con Tensorflow 2.0 Keras API\n",
    "\n",
    "![EstructuraUrl](https://cdn-images-1.medium.com/fit/t/1600/480/1*cozITwlOo6tzXSWrSxv_Cg.jpeg)\n",
    "\n",
    "Vamos a implementar el modelo de deep learning súper basico en la librería Tensorflow con API de Keras\n",
    "\n",
    "Primero vamos a correr los siguientes comandos en la consola:\n",
    "\n",
    "Primero vamos a correr los siguientes comandos en nuestra consola\n",
    "```python\n",
    "pip3 install scikit-learn\n",
    "pip3 install pandas\n",
    "pip3 install numpy\n",
    "pip3 install tensorflow\n",
    "```\n",
    "En mi caso lo corro aqui en jupyter notebook\n",
    "\n",
    "El dataset (un conjunto de datos) del Titanic que hablamos inicialmente lo descargamos de [AQUÍ](https://www.kaggle.com/c/titanic). Deberían Registrarse a KAGGLE es sencillo y luego entrar en la sección data y descargar todos los datos.\n",
    "Una vez descargado, descomprime los archivos, dentro de la carpeta donde tengas tu código.\n",
    "Ahora vamos a codificar paso a paso.\n",
    "Vale aclarar que el csv es una archivo que contiene textos o valores separados por comas, comma separated values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Vamos a empezar importando las librerías que vamos a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Ahora vamos a cargar el dataset que contiene nuestro .csv específicamente el archivo train.csv para ello utilizamos pandas. Esto nos permitirá además poder ver que variables no son importantes para nuestro entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/josebenitez/Documents/test_ANN/titanic/train.csv')\n",
    "df.head() #esta función head nos permite mostrar los primeros 5 datos de nuestro datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí solo con la vista podemos descartar muchas variables que son las siguientes\n",
    "- Nombre: No nos sirve porque no aporta información al resultado\n",
    "- Ticket: Tampoco nos aporta Información\n",
    "- Cabin: Podría aportarnos información pero tiene valores perdidos\n",
    "- PassangerID: Es el indice, eso no aporta ni una información relevante\n",
    "- Parch: Lo eliminaremos para tener una peor rendimiento para saber como optimizar el modelo en la siguiente sección\n",
    "\n",
    "Entonces los datos de entrada X van a tener los siguientes valores:\n",
    "- Sex\n",
    "- Age\n",
    "- SibSp: cantidad de hermanos o esposa\n",
    "- Fare: impuesto que pagan\n",
    "- Embarked: es la clase donde embarcó\n",
    "\n",
    "Con eso ya podemos separa la nuestra variable independiente de la dependiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Pclass', 'Sex', 'Age', 'Embarked', 'Fare', 'SibSp'])\n",
    "\n",
    "X = df[['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']]\n",
    "y = df.Survived.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lo que vamos a ver continuación, un preprocesado de los datos, es tal vez un poco abstracto. Es decir, cuesta un poco entender rápidamente pero voy a tratar de simplificarlo todo lo que pueda. Bueno, como vieron arriba tenemos varios tipos de datos que tienen un rango muy alto o muy bajo entre uno y otro por ejemplo el de las edades, porque al pasar por mi función de activación, ésta no nos entregará resutados óptimos ya que va a tener muchas fluctuaciones. Lo que haremos es normalizar estos datos, que queden en una escala donde no tengan mucha diferencia entre valores.\n",
    "\n",
    "Entonces lo que hacemos es separar los valores primero, sabemos que Sex, Pclass y Embarked son valores que pertenecen a categorías, es decir, para las matemáticas no es más ni menos los que están en la primera clase que en la segunda.\n",
    "Pero en el caso de Age y Fare no podemos darle una categoría directamente porque son valores que varían, para ello vamos a utilizar la función StandardScaler para poder tenerlos en una misma escala como lo mencioné más arriba. Les recomiendo que estudien estas librerías ya que no quiero desviar el tema que estamos tratando.\n",
    "\n",
    "Les dejo igual una explicación que encontré en StackOverflow y me gustó mucho:\n",
    ">La idea detrás de StandardScaler es que transformará sus datos de manera que su distribución tenga un valor medio 0 y una desviación estándar de 1.\n",
    "En el caso de datos multivariados, esto se hace en función de las características (en otras palabras, de forma independiente para cada columna de datos).\n",
    "Dada la distribución de los datos, cada valor en el conjunto de datos tendrá el valor medio restado, y luego dividido por la desviación estándar de todo el conjunto de datos (o característica en el caso multivariante).\n",
    "\n",
    "Marea un poco la expliación fácil :P . Pero no se preocupen, leyendo y releyendo se termina aprendiendo. Me pasó lo mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocesador = make_column_transformer(\n",
    "    (StandardScaler(), ['Age','Fare']),\n",
    "    (OneHotEncoder(), ['Sex','Pclass','Embarked'])\n",
    ")\n",
    "\n",
    "X = preprocesador.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Ahora armaremos el perceptron, para ello importamos las neuronas simples y el modo secuencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a construir el modelo inicial, la primera capa Dense recibe el número de datos por fila y como activación utilzaremos la función rectificadora. La capa de salida debe tener la misma dimensión como cantidad de salidas necesito. En este caso solo tengo 1 o 0 como salida por lo que aplico la función sigmoide en la salida para calcular la probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a compilar y que el error lo trate como una salida binaria, el optimizador será nuestra función derivada que nos permite saber hacia donde mover los pesos. Vamos a realizar 100 pasos y el batch_size nos dice cuantas veces vamos a pasar por los datos para actualizar los pesos. Esto es muy importante definir y depende de la capacidad y memoria de cada computadora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6724 - acc: 0.6081\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6719 - acc: 0.6081\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6721 - acc: 0.6081\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6717 - acc: 0.6081\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6718 - acc: 0.6081\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6723 - acc: 0.6081\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6721 - acc: 0.6081\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6716 - acc: 0.6081\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6717 - acc: 0.6081\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6712 - acc: 0.6081\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6713 - acc: 0.6081\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6711 - acc: 0.6081\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 0s 989us/step - loss: 0.6711 - acc: 0.6081\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6708 - acc: 0.6081\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6709 - acc: 0.6081\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6705 - acc: 0.6081\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 0s 951us/step - loss: 0.6708 - acc: 0.6081\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6706 - acc: 0.6081\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6706 - acc: 0.6081\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6705 - acc: 0.6081\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6703 - acc: 0.6081\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6705 - acc: 0.6081\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6700 - acc: 0.6081\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6707 - acc: 0.6081\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6704 - acc: 0.6081\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6699 - acc: 0.6081\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6698 - acc: 0.6081\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6698 - acc: 0.6081\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6694 - acc: 0.6081\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6692 - acc: 0.6081\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6696 - acc: 0.6081\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6691 - acc: 0.6081\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 0s 996us/step - loss: 0.6690 - acc: 0.6081\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6694 - acc: 0.6081\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6692 - acc: 0.6081\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6697 - acc: 0.6081\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6685 - acc: 0.6081\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 0s 964us/step - loss: 0.6695 - acc: 0.6081\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6692 - acc: 0.6081\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6682 - acc: 0.6081\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6687 - acc: 0.6081\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6687 - acc: 0.6081\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6686 - acc: 0.6081\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6687 - acc: 0.6081\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6682 - acc: 0.6081\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6681 - acc: 0.6081\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6680 - acc: 0.6081\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 0s 977us/step - loss: 0.6687 - acc: 0.6081\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6678 - acc: 0.6081\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 0s 979us/step - loss: 0.6678 - acc: 0.6081\n",
      "Epoch 51/100\n",
      "9/9 [==============================] - 0s 979us/step - loss: 0.6681 - acc: 0.6081\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6681 - acc: 0.6081\n",
      "Epoch 53/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6671 - acc: 0.6081\n",
      "Epoch 54/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6678 - acc: 0.6081\n",
      "Epoch 55/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6678 - acc: 0.6081\n",
      "Epoch 56/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6673 - acc: 0.6081\n",
      "Epoch 57/100\n",
      "9/9 [==============================] - 0s 850us/step - loss: 0.6671 - acc: 0.6081\n",
      "Epoch 58/100\n",
      "9/9 [==============================] - 0s 941us/step - loss: 0.6669 - acc: 0.6081\n",
      "Epoch 59/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6671 - acc: 0.6081\n",
      "Epoch 60/100\n",
      "9/9 [==============================] - 0s 957us/step - loss: 0.6674 - acc: 0.6081\n",
      "Epoch 61/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6670 - acc: 0.6081\n",
      "Epoch 62/100\n",
      "9/9 [==============================] - 0s 814us/step - loss: 0.6667 - acc: 0.6081\n",
      "Epoch 63/100\n",
      "9/9 [==============================] - 0s 888us/step - loss: 0.6671 - acc: 0.6081\n",
      "Epoch 64/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6670 - acc: 0.6081\n",
      "Epoch 65/100\n",
      "9/9 [==============================] - 0s 738us/step - loss: 0.6670 - acc: 0.6081\n",
      "Epoch 66/100\n",
      "9/9 [==============================] - 0s 954us/step - loss: 0.6666 - acc: 0.6081\n",
      "Epoch 67/100\n",
      "9/9 [==============================] - 0s 971us/step - loss: 0.6662 - acc: 0.6081\n",
      "Epoch 68/100\n",
      "9/9 [==============================] - 0s 845us/step - loss: 0.6662 - acc: 0.6081\n",
      "Epoch 69/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6660 - acc: 0.6081\n",
      "Epoch 70/100\n",
      "9/9 [==============================] - 0s 768us/step - loss: 0.6665 - acc: 0.6081\n",
      "Epoch 71/100\n",
      "9/9 [==============================] - 0s 764us/step - loss: 0.6661 - acc: 0.6081\n",
      "Epoch 72/100\n",
      "9/9 [==============================] - 0s 865us/step - loss: 0.6659 - acc: 0.6081\n",
      "Epoch 73/100\n",
      "9/9 [==============================] - 0s 712us/step - loss: 0.6652 - acc: 0.6081\n",
      "Epoch 74/100\n",
      "9/9 [==============================] - 0s 971us/step - loss: 0.6657 - acc: 0.6081\n",
      "Epoch 75/100\n",
      "9/9 [==============================] - 0s 711us/step - loss: 0.6660 - acc: 0.6081\n",
      "Epoch 76/100\n",
      "9/9 [==============================] - 0s 936us/step - loss: 0.6659 - acc: 0.6081\n",
      "Epoch 77/100\n",
      "9/9 [==============================] - 0s 921us/step - loss: 0.6659 - acc: 0.6081\n",
      "Epoch 78/100\n",
      "9/9 [==============================] - 0s 894us/step - loss: 0.6656 - acc: 0.6081\n",
      "Epoch 79/100\n",
      "9/9 [==============================] - 0s 968us/step - loss: 0.6653 - acc: 0.6081\n",
      "Epoch 80/100\n",
      "9/9 [==============================] - 0s 813us/step - loss: 0.6650 - acc: 0.6081\n",
      "Epoch 81/100\n",
      "9/9 [==============================] - 0s 841us/step - loss: 0.6653 - acc: 0.6081\n",
      "Epoch 82/100\n",
      "9/9 [==============================] - 0s 957us/step - loss: 0.6655 - acc: 0.6081\n",
      "Epoch 83/100\n",
      "9/9 [==============================] - 0s 694us/step - loss: 0.6652 - acc: 0.6081\n",
      "Epoch 84/100\n",
      "9/9 [==============================] - 0s 990us/step - loss: 0.6654 - acc: 0.6081\n",
      "Epoch 85/100\n",
      "9/9 [==============================] - 0s 849us/step - loss: 0.6649 - acc: 0.6081\n",
      "Epoch 86/100\n",
      "9/9 [==============================] - 0s 778us/step - loss: 0.6650 - acc: 0.6081\n",
      "Epoch 87/100\n",
      "9/9 [==============================] - 0s 963us/step - loss: 0.6645 - acc: 0.6081\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 819us/step - loss: 0.6643 - acc: 0.6081\n",
      "Epoch 89/100\n",
      "9/9 [==============================] - 0s 882us/step - loss: 0.6644 - acc: 0.6081\n",
      "Epoch 90/100\n",
      "9/9 [==============================] - 0s 923us/step - loss: 0.6642 - acc: 0.6081\n",
      "Epoch 91/100\n",
      "9/9 [==============================] - 0s 779us/step - loss: 0.6642 - acc: 0.6081\n",
      "Epoch 92/100\n",
      "9/9 [==============================] - 0s 909us/step - loss: 0.6639 - acc: 0.6081\n",
      "Epoch 93/100\n",
      "9/9 [==============================] - 0s 830us/step - loss: 0.6647 - acc: 0.6081\n",
      "Epoch 94/100\n",
      "9/9 [==============================] - 0s 729us/step - loss: 0.6645 - acc: 0.6081\n",
      "Epoch 95/100\n",
      "9/9 [==============================] - 0s 900us/step - loss: 0.6643 - acc: 0.6081\n",
      "Epoch 96/100\n",
      "9/9 [==============================] - 0s 718us/step - loss: 0.6643 - acc: 0.6081\n",
      "Epoch 97/100\n",
      "9/9 [==============================] - 0s 966us/step - loss: 0.6638 - acc: 0.6081\n",
      "Epoch 98/100\n",
      "9/9 [==============================] - 0s 817us/step - loss: 0.6637 - acc: 0.6081\n",
      "Epoch 99/100\n",
      "9/9 [==============================] - 0s 878us/step - loss: 0.6631 - acc: 0.6081\n",
      "Epoch 100/100\n",
      "9/9 [==============================] - 0s 929us/step - loss: 0.6630 - acc: 0.6081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14ab94670>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ya entrenamos nuestro modelo, vamos a crear una pequeña función donde le pasamos algunas variable de entreda como Pclass, Sex, Age, Fare, Emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(Pclass=1, Sex='female', Age=60 ,Fare=0, Embarked='C'):\n",
    "    cnames = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']\n",
    "    data = [[Pclass, Sex, Age, Fare, Embarked]]\n",
    "    my_X = pd.DataFrame(data=data, columns=cnames)\n",
    "    my_X = preprocesador.transform(my_X)\n",
    "    return model.predict_classes(my_X)\n",
    "\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
