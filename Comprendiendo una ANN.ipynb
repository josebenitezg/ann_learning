{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Plasmando en c칩digo el funcionamiento de las Redes Neuronales Artificiales - Parte 1</h1>\n",
    "\n",
    "### Para que podamos entrar en contexto sobre nuestro tema principal, primero les quiero hacer hacer una peque침a explicaci칩n de c칩mo funciona una neurona simple en el ser humano, cuyo compratmiento es el que queremos imitar en una m치quina.\n",
    "\n",
    "\n",
    "En este m칩dulo aprenderemos los conecptos b치sicos y las noticiones aritm칠ticas y estadistas de c칩mo funciona una red neuronal artificial as칤 como entener con un c칩digo simple su funcionamiento y luego utilizar librer칤as conocidas como TensorFlow 2.0\n",
    "\n",
    "## 1. La Neurona Humana\n",
    "![NeuronUrl](https://media.giphy.com/media/5h9EHCvA0OR2/giphy.gif \"neuron\")\n",
    "\n",
    "En el gif podemos apreciar las redes nueronales y lo incre칤ble de esto es poder entender c칩mo imitar el compartamiento de una neurona humana con la de una m치quina 쯏 por qu칠 hacemos esto? es simple, nuestro cerebro humano es una m치quina compleja capaz de resolver operaciones -que para una computadora parece muy compleja- con much칤sima facilidad. Imaginate poder recrear, la informaci칩n que recibimos con nuestros ojos desde una c치mara, la percepci칩n de colores, procesarlos y obtener informaci칩n para que realice una tarea que nos ayude. Imitar el funcionamiento del cerebro humano nos ayuda a extender nuestras posibilidades como seres humanos.\n",
    "\n",
    "Entonces nuestro desaf칤o principal est치 en como recrear una neurona. Para ello, vamos a ver analizar su funcionamiento.\n",
    "\n",
    "![RamonUrl](https://supercurioso.com/wp-content/uploads/2014/12/ramonycajalelartistadelaneurona.jpg \"ramon\")\n",
    "\n",
    "En el a침o 1899 Santiago Ram칩n y Cajal (imagen derecha) lo que hizo fue te침ir las neuronas y observarlas bajo el microscopio y traz칩 la imagen de lo que el cre칤a se trataba de una neurona individual (imagen izquierda).\n",
    "\n",
    "A침os m치s tarde, con la llegada de nuevas tecnolog칤as permitieron una mejor observaci칩n y como mostraremos en la imagen m치s abajo, una estructura real y gr치fica de la neurona es muy parecida a la imagen de Santiago.\n",
    "\n",
    "![EstructuraUrl](https://respuestas.tips/wp-content/uploads/2013/10/partes-de-una-neurona.jpg \"estructura\")\n",
    "\n",
    "En realidad, es mucho m치s compleja en cuanto a todas las partes que la componen, pero quiero que nos enfoquemos en tres partes importantes de la neurona, las dendritas, el ax칩n y el n칰cleo.\n",
    "\n",
    "Lo que tenemos que entender aqu칤 es que las neuronas por s칤 solas no sirven, deben convivir con otras para su correcto funcionamiento para lograr cosas incre칤bles que ya conocemos.\n",
    "\n",
    "쮺칩mo trabajan juntas?\n",
    "\n",
    "Bueno, las dendritas, reciben la informaci칩n, pasa por el n칰cleo y el ax칩n se encarga de llevar esta informaci칩n a otra neurona, b치sicamente transmiten impulsos el칠ctricos desde el ax칩n a las dendritas de otra neurona y este proceso es al que se le conoce como sinapsis y para que se hagan una idea nuestra cerebro en edad adulta cuenta con 86 mil millones de neuronas en promedio.\n",
    "\n",
    "Ahora, vamos a empezar a estudiar como logramos plasmar la neurona humana en una m치quina.\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/642/1*ifwiDM2fRKze9VBG2p6_Bg.png \"estructura\")\n",
    "\n",
    "Entre las d칠cadas del 1950 y 1960 el psicol칩go y cient칤fico estadounidense, inspirado en el trabajo de otros cient칤ficos, crea algo que hoy conocemos como perceptr칩n (imagen de arriba). Que es la 칰nidad b치sica de inferencia o una neurona artificial.\n",
    "\n",
    "Ahora imaginemos a las fechas como dendritas y las X como informaci칩n que le pasa otra neurona, entonces las flechas ser칤an las sinapsis. En este caso la se침al que le pasamos, vendr칤a a ser una valor de entrada y a partir de estos valores generan un valor de salida. A las entradas les llamamos variables independientes porque pueden ser cualquier valor por separado y a la salida como variable dependiente porque es un resultado que depende de las entradas. Y a las la sinapsis que vemos ahora los llamaremos pesos \"w\" como la imagen de abajo:\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/642/1*ifwiDM2fRKze9VBG2p6_Bg.png \"estructura\")\n",
    "\n",
    "Esta sinapsis que ahora le llamaremos pesos o com칰nmente representado por \"w\" son fundamentales para el correcto funcionamiento de nuestro perceptr칩n porque estos valores de pesos se utilizan para que pueda aprender qu칠 valor de entrada a la neuora es importante y que no. De hecho lo 칰nico que ajustamos cuando entrenamos una red neuronal es ajustar los valores de los pesos constantemente, pero 쯣or qu칠?\n",
    "\n",
    "El entrenamiento tiene dos pasos\n",
    "\n",
    "    Primer paso:\n",
    "    El entrenamiento lo que hace es una suma total del valor de entrada multiplicado por los pesos es decir: x1.w1 + x2.w2 + xn*wn. A esto se le conoce como suma ponderada y podemos representarlo con la siguiente ecuaci칩n:\n",
    "    \n",
    "   $$\\sum\\limits_{i=0}^{m} {x_i*w_i}$$  \n",
    "    \n",
    "    Segundo paso:\n",
    "    A nuestra suma ponderada le aplicamos una funci칩n de activaci칩n delta (no se preocupen veremos a continuaci칩n que es esta funci칩n). Quedando as칤 finalmente \n",
    "    \n",
    "   $$\\delta(\\sum\\limits_{i=0}^{m} {x_i*w_i})$$ \n",
    "\n",
    "\n",
    "## 2. Funci칩n de activaci칩n\n",
    "\n",
    "La funci칩n de activaci칩n es la que me permite definir el comportamiento de mi neurona y de cada capa, esta funci칩n decide que valor pasa o se queda. Hablaremos brevemente de 4 de ellas.\n",
    "\n",
    "- Threshold Function o escal칩n unitario\n",
    "![EstructuraUrl](https://miro.medium.com/max/960/0*etGrZj_m0spvgN9n.png \"estructura\")\n",
    "Esta es la funci칩n m치s simple, como ver치n cuando recibe valore negativos (menores a 0) la funci칩n est치 en cero y cuando recibe valores positivos (mayores a 0) la funci칩n pasa a ser 1, es decir nuestro valor de salida siempre va a ser 1 y 0 nada m치s. Ahora que comprendemos como es una funci칩n de activaci칩n b치sica vamos a uno un poco m치s compleja.\n",
    "\n",
    "\n",
    "- Funci칩n Sigmoide\n",
    "![EstructuraUrl](https://miro.medium.com/max/744/1*zSDNMn4z_hmYing0wDCSoQ.png \"sigmoide\")\n",
    "Como vemos esta funci칩n es un poco m치s suave, pasa por todos los valores entre 0 y 1, es muy 칰ltil cuando queremos calcular probabilidades en nuestra salida.\n",
    "\n",
    "\n",
    "- Funci칩n Rectificadora\n",
    "![EstructuraUrl](https://miro.medium.com/max/736/1*RthaUGkni6YCtwlAKsadsg.png \"recti\")\n",
    "Es una de las funciones m치s utilizadas en Machine Learning, esto lo que hace simplemente es entregarnos los valores cuando se치n mayores a cero, es decir, valor positivo. Para cada valor positivo un valor de la curva.\n",
    "\n",
    "\n",
    "- Tangente Hiperb칩lica\n",
    "![EstructuraUrl](https://miro.medium.com/max/772/1*sYnLa9djWZ7BozqUV5lNmw.png \"hiper\")\n",
    "Esta funci칩n es muy similar a la Sigmoide pero va desde -1 a 1 los valores.\n",
    "\n",
    "Quisiera que puedas tener en mente esto ya que todav칤a no entraremos a profundidad pero si los veremos m치s adelante.\n",
    "\n",
    "## 3. 쮺칩mo funcionan las Redes Neuronales Artificiales en un caso real?\n",
    "\n",
    "Vamos a imaginar que hacemos una clasificaci칩n, vamos a usar un dataset, en este caso y para ejemplo el de Titanic, donde nos trae una lista de pasajeros y si sobrevivieron o no. A continuaci칩n vamos a representar nuestro perceptr칩n con las variables y se ve as칤:\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/1084/1*_pD95maLHcEL8Hc39EMJGA.png \"ti\")\n",
    "\n",
    "Para este caso es s칰per f치cil calcular ya que el resultado es simplemente la suma ponderada con la funci칩n de activaci칩n. Pero que pasa si a침ado m치s capacas de a nuestro modelo, es decir, todas las entradas iran conectadas a la capa siguiente como vemos en la siguiente imagen, eso significa que podr칤amos predecir m치s valores, aunque no piensen que si tenemos m치s capa mejora el resultado.\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/1084/1*YjtSTddpDPCscJSXcQblBg.png \"j\")\n",
    "\n",
    "Mientras m치s capas tengas m치s nivel de abstracci칩n va a tener nuestra red. Recuerden que b치sicamente cada una recibe un valor de peso distinto por lo que puede o no recibir esa entrada x en algunos perceptrones, de esta manera, cada uno realiza una funci칩n difrente, aunque no siempre sea as칤.\n",
    "\n",
    "## 4. 쮺칩mo aprenden las redes neuronales?\n",
    "\n",
    "En programaci칩n hay muchas maneras de llegar a una soluci칩n, por ejemplo, podr칤amos programar todos los casos y respuestas para una soluci칩n posible. Cuando esta tarea lleva variables que no siempre se van a comportar como el algoritmo espera, por ejemplo. Queremos crear algo que reconzca un perro en cualquier fotos in importar la raza. Suena sencillo, clasifico las imagenes, digo que son perros, y listo. Pero esta tarea se complica ya que existen n razas de perros, pueden salir en n tipo de ambientes en la fotograf칤a y con diferentes tipos de poses, para esto, es mucho m치s complicado tener un c칩digo que lo resuleva o programar cada caso, para ello se crean las redes neuronales artificiales, claro, para nosotros es natural saber que hay un perro en la foto pero para la m치quina no.\n",
    "\n",
    "La ventaja de las redes neuronales es que puede \"entender\" los datos que le damos de entrada y hacer una funci칩n para cualquier entrada que le presentemos, y como buen ejemplo hemos dado la imagen del perro.\n",
    "\n",
    "Veamos de nuevo la red neuronal anterior...\n",
    "![EstructuraUrl](https://miro.medium.com/max/1084/1*YjtSTddpDPCscJSXcQblBg.png \"j\")\n",
    "Lo que vamos a hacer es calcular el valor de la salida \"y\" (si muri칩 o no) en base a nuestro dato de entrada \"x\" (variables que vemos arriba). Adem치s de esto le vamos a pasar a Y valores reales de que si muri칩 o no. 쯇ero entonces de que me sirve la predicci칩n en \"y\" si le paso los valores reales?. Es simple, al final de cada paso por la neurona, la m치quina lo que va a a hacer es comparar el valor de salida de la predicci칩n con el valor real, esto se hace a trav칠s de la funci칩n de p칠rdida y podemos verlo en la siguiente ecuaci칩n:\n",
    "$$L = {1 \\over 2}(\\Upsilon - y)^2$$\n",
    "Aqu칤 Upsilon (la y medio rara que se ve ah칤 jajaja) es nuestro valor predecido y la y normal es nuestro valor real. Esta es la m치s com칰n entre las funciones de p칠rdida y esta se llama \"Mean Squered Error\" o la medidad de los errores al cuadrado. Esta funci칩n lo que nos dice es cu치nto es el error que va a tener nuestra red y lo que tenemos que hacer es minimizarla.\n",
    "Esta funci칩n va para atr치s y corrije los pesos para ir minizando los pesos \"w\".\n",
    "\n",
    "## 5. 쮺칩mo es que minimizo mi funci칩n de perdida?\n",
    "\n",
    "Bueno para poder minizar la funci칩n de perdida podr칤amos utilizar algo que se llama el descenso de gradiente (aclaro que este texto no muestra la unica forma pero si trato de simplificar lo m치ximo posible para adentrarnos en este complejo mundo).\n",
    "Lo que hace esta funci칩n es tratar de encontrar los valores de los pesos de modo a que el error se ajuste con la pendiente de la recta dentro del conjunto de valores (puntos azules) que ser칤an las entradas como se aprecia en el siguiente gif:\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/proxy/1*KQVi812_aERFRolz_5G3rA.gif)\n",
    "\n",
    "Pero que pasa si nuestra funci칩n no representa una parabola y tiene varios m칤nimos como se ve a continuaci칩n\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/1400/0*XKMTnu8MwD2wdso6.png)\n",
    "\n",
    "Si nuestro algoritmo solamente encuentra un m칤nimo local no encontrar치 un m칤nimo global que es el que neceitamos para que se ajusten los valores de los pesos.\n",
    "\n",
    "Entonces con el descendo de gradiente lo que nuestra funci칩n hace en cada paso es\n",
    "- Paso el dato 1\n",
    "- Paso el dato 2\n",
    "- Paso el dato 3\n",
    "- Ajusto los pesos\n",
    "\n",
    "Para solucionar el problema anterior donde tenemos varios m칤nimos posibles se utiliza otro algoritmo que hace algo que se llama descenso de gradiente estoc치stico y hace lo siguiente\n",
    "- Paso el dato 1\n",
    "- Ajusto los pesos\n",
    "- Paso el dato 2\n",
    "- Ajusto los pesos\n",
    "- Paso el dato 3\n",
    "- Ajusto los pesos\n",
    "\n",
    "Entonces cada vez que pase por el dato va a justar los pesos y se acercar치 m치s al m칤nimo global ya que se mueve constantemente ante cada paso de datos.\n",
    "\n",
    "## 5. Backpropagation\n",
    "\n",
    "Todo lo que vimos hasta ahora es lo que se llama comunmente Forward Propagation, donde el valor de entrada, pasa por toda la red, pasa por la funci칩n de activaci칩n, se calcula la funci칩n de p칠rdida, luego de este proceso la p칠rdida se debe \"propagar hacia atr치s\" ajustando los valores de los pesos, esto es lo que hace el backpropagation, ir para atr치s y ajustar los valores de los pesos a la vez en simult치neo, es un algoritmo con unas matem치ticas muy avanzadas e interesantes que los puedes mirar por tu cuenta si quieres :)\n",
    "\n",
    "## 6. Manos a la obra\n",
    "\n",
    "Ahora lo que haremos es aplicar una peque침a red neuronal sin utilizar ninguna librer칤a especial excepto la de numpy. Numpy es una liber칤a de python que nos permite realizar operaciones matem치ticas complejas con una f치cil implementaci칩n del c칩digo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Como primer paso lo que haremos es instalar esta liber칤a en caso de que nuestra PC no lo tenga, si est치s usando alg칰n otro editor puedes instalarlo desde tu terminal quitando el signo de exclamaci칩n que tiene en frente.\n",
    "\n",
    "Recuerden que para ejecutar un bloque de c칩digo como el que ven abajo solo deben ubicar el cursos sobre el bloque y presionar SHIFT+ENTER y as칤 sucesivamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.18.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Importamos la librer칤a de numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lo que hacemos en este punto crear una funci칩n que calcule nuestra funci칩n de activaci칩n, que ser칤a una variante de la funci칩n sigmoide.\n",
    "Tengamos en cuenta que creamos esta funci칩n para usarlo en sus dos formas, si queremos como sus derivadas o no.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlin(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lo que hacemos aqu칤 abajo es crear dos matrices, una matriz \"x\" que ya sabemos que es la entrada \"y\" que son los valores reales de la salida osea los resultados reales.\n",
    "\n",
    "Recuerden que esos datos que presentamos en la entrada son valores cualquiera, solo representan una entrada que queremos que nos brinde una salida predictiva.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,0,1],\n",
    "      [0,1,1],\n",
    "      [1,0,1],\n",
    "      [1,1,1]])\n",
    "y = np.array([[0],\n",
    "      [1],\n",
    "      [1],\n",
    "      [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Aqu칤 lo que haremos es crear valores de pesos de manera randomica, que la computadora me los genere por que es lo que voy a ir corrigiendo con el algoritmo. Los imprimo para que los veamos.\n",
    "Supongamos que vamos a tener dos capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
      " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n",
      " [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n",
      "[[-0.5910955 ]\n",
      " [ 0.75623487]\n",
      " [-0.94522481]\n",
      " [ 0.34093502]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "w1 = 2*np.random.random((3,4))-1\n",
    "w2 = 2*np.random.random((4,1))-1\n",
    "\n",
    "print(w1)\n",
    "print(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bueno aqu칤 empieza lo divertido 游땕\n",
    "Supogamos que L1 y L2 son las capas.\n",
    "Lo que vamos a hacer primero es el #Forward Propagation, en este paso primero guardamos en \"L0\" nuestros valores de \"x\" y luego hacemos un producto punto con los valores de los pesos (suma ponderarda) y lo pasamos por la funci칩n de activaci칩n como se puestra en L1, estos valores se multiplican con los valores anteriores y los pesos de la capa 2.\n",
    "\n",
    "Luego en #C치lculo de erro lo que hacemos es calcular nuestro error de los valores reales con nuestras predicciones para hallar el valor de p칠rdida. Esto hacemos restando el valor de real con el valor que se predijo.\n",
    "\n",
    "Recuerdan que hablamos sobre el algoritmo de descenso de gradiente?. Bueno eso se calcula multiplicando el error actual por la derivada de la funci칩n que creamos con las predicciones actuales (por eso decidimos tener una opci칩n de derivada m치s arriba) Recuerde la derivada es la recta tangente en un punto a la funci칩n.\n",
    "\n",
    "#Backpropagation.. Una vez que tengamos el descenso de gradiente (la pendiente) multimplicamos por la transpuesta de los pesos de la capa siguiente y de este modo propagaremos el error hacia atr치s.\n",
    "\n",
    "Y finalmente #ajustamos el valor de los pesos con la transpuesta de las capas por sus respectivas optimizaciones o direcciones hacia donde actualizar los pesos y esto suman a nuestros pesos actuales modificando los dos a la vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(6000):\n",
    "    #Forward Propagation\n",
    "    L0 = x\n",
    "    L1 = nonlin(np.dot(L0,w1))\n",
    "    L2 = nonlin(np.dot(L1,w2)) \n",
    "    \n",
    "    #C치lculo de error\n",
    "    L2_error = y - L2       \n",
    "    \n",
    "    #Calculamos el descenso de gradiente\n",
    "    L2_delta = L2_error*nonlin(L2,deriv=True)\n",
    "    \n",
    "    #Backpropagation\n",
    "    L1_error = L2_delta.dot(w2.T)\n",
    "    L1_delta = L1_error * nonlin(L1,deriv=True)\n",
    "    \n",
    "    #ajustamos el valor de los pesos\n",
    "    w2 += L1.T.dot(L2_delta)\n",
    "    w1 += L0.T.dot(L1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora analicemos los resultados, vemos como aqu칤 nuestro valores \"y\" que son los reales vs los valores que la m치quina predijo que ser칤an. 쯄uy loco no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores reales\n",
      "\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "\n",
      "Valores de las predicciones\n",
      "\n",
      "[[0.00631758]\n",
      " [0.99192018]\n",
      " [0.99291766]\n",
      " [0.0094266 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Valores reales\\n\")\n",
    "print(y)\n",
    "print(\"\\n\")\n",
    "print(\"Valores de las predicciones\\n\")\n",
    "print(L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Ahora s칤, el dataset de Titanic con Tensorflow 2.0 Keras API\n",
    "\n",
    "![EstructuraUrl](https://cdn-images-1.medium.com/fit/t/1600/480/1*cozITwlOo6tzXSWrSxv_Cg.jpeg)\n",
    "\n",
    "Vamos a implementar el modelo de deep learning s칰per basico en la librer칤a Tensorflow con API de Keras\n",
    "\n",
    "Primero vamos a correr los siguientes comandos en la consola:\n",
    "\n",
    "Primero vamos a correr los siguientes comandos en nuestra consola\n",
    "```python\n",
    "pip3 install scikit-learn\n",
    "pip3 install pandas\n",
    "pip3 install numpy\n",
    "pip3 install tensorflow\n",
    "```\n",
    "En mi caso lo corro aqui en jupyter notebook\n",
    "\n",
    "El dataset (un conjunto de datos) del Titanic que hablamos inicialmente lo descargamos de [AQU칈](https://www.kaggle.com/c/titanic). Deber칤an Registrarse a KAGGLE es sencillo y luego entrar en la secci칩n data y descargar todos los datos.\n",
    "Una vez descargado, descomprime los archivos, dentro de la carpeta donde tengas tu c칩digo.\n",
    "Ahora vamos a codificar paso a paso.\n",
    "Vale aclarar que el csv es una archivo que contiene textos o valores separados por comas, comma separated values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Vamos a empezar importando las librer칤as que vamos a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Ahora vamos a cargar el dataset que contiene nuestro .csv espec칤ficamente el archivo train.csv para ello utilizamos pandas. Esto nos permitir치 adem치s poder ver que variables no son importantes para nuestro entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/josebenitez/Documents/test_ANN/titanic/train.csv')\n",
    "df.head() #esta funci칩n head nos permite mostrar los primeros 5 datos de nuestro datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqu칤 solo con la vista podemos descartar muchas variables que son las siguientes\n",
    "- Nombre: No nos sirve porque no aporta informaci칩n al resultado\n",
    "- Ticket: Tampoco nos aporta Informaci칩n\n",
    "- Cabin: Podr칤a aportarnos informaci칩n pero tiene valores perdidos\n",
    "- PassangerID: Es el indice, eso no aporta ni una informaci칩n relevante\n",
    "- Parch: Lo eliminaremos para tener una peor rendimiento para saber como optimizar el modelo en la siguiente secci칩n\n",
    "\n",
    "Entonces los datos de entrada X van a tener los siguientes valores:\n",
    "- Sex\n",
    "- Age\n",
    "- SibSp: cantidad de hermanos o esposa\n",
    "- Fare: impuesto que pagan\n",
    "- Embarked: es la clase donde embarc칩\n",
    "\n",
    "Con eso ya podemos separa la nuestra variable independiente de la dependiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Pclass', 'Sex', 'Age', 'Embarked', 'Fare', 'SibSp'])\n",
    "\n",
    "X = df[['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']]\n",
    "y = df.Survived.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lo que vamos a ver continuaci칩n, un preprocesado de los datos, es tal vez un poco abstracto. Es decir, cuesta un poco entender r치pidamente pero voy a tratar de simplificarlo todo lo que pueda. Bueno, como vieron arriba tenemos varios tipos de datos que tienen un rango muy alto o muy bajo entre uno y otro por ejemplo el de las edades, porque al pasar por mi funci칩n de activaci칩n, 칠sta no nos entregar치 resutados 칩ptimos ya que va a tener muchas fluctuaciones. Lo que haremos es normalizar estos datos, que queden en una escala donde no tengan mucha diferencia entre valores.\n",
    "\n",
    "Entonces lo que hacemos es separar los valores primero, sabemos que Sex, Pclass y Embarked son valores que pertenecen a categor칤as, es decir, para las matem치ticas no es m치s ni menos los que est치n en la primera clase que en la segunda.\n",
    "Pero en el caso de Age y Fare no podemos darle una categor칤a directamente porque son valores que var칤an, para ello vamos a utilizar la funci칩n StandardScaler para poder tenerlos en una misma escala como lo mencion칠 m치s arriba. Les recomiendo que estudien estas librer칤as ya que no quiero desviar el tema que estamos tratando.\n",
    "\n",
    "Les dejo igual una explicaci칩n que encontr칠 en StackOverflow y me gust칩 mucho:\n",
    ">La idea detr치s de StandardScaler es que transformar치 sus datos de manera que su distribuci칩n tenga un valor medio 0 y una desviaci칩n est치ndar de 1.\n",
    "En el caso de datos multivariados, esto se hace en funci칩n de las caracter칤sticas (en otras palabras, de forma independiente para cada columna de datos).\n",
    "Dada la distribuci칩n de los datos, cada valor en el conjunto de datos tendr치 el valor medio restado, y luego dividido por la desviaci칩n est치ndar de todo el conjunto de datos (o caracter칤stica en el caso multivariante).\n",
    "\n",
    "Marea un poco la expliaci칩n f치cil :P . Pero no se preocupen, leyendo y releyendo se termina aprendiendo. Me pas칩 lo mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocesador = make_column_transformer(\n",
    "    (StandardScaler(), ['Age','Fare']),\n",
    "    (OneHotEncoder(), ['Sex','Pclass','Embarked'])\n",
    ")\n",
    "\n",
    "X = preprocesador.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Ahora armaremos el perceptron, para ello importamos las neuronas simples y el modo secuencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a construir el modelo inicial, la primera capa Dense recibe el n칰mero de datos por fila y como activaci칩n utilzaremos la funci칩n rectificadora. La capa de salida debe tener la misma dimensi칩n como cantidad de salidas necesito. En este caso solo tengo 1 o 0 como salida por lo que aplico la funci칩n sigmoide en la salida para calcular la probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a compilar y que el error lo trate como una salida binaria, el optimizador ser치 nuestra funci칩n derivada que nos permite saber hacia donde mover los pesos. Vamos a realizar 100 pasos y el batch_size nos dice cuantas veces vamos a pasar por los datos para actualizar los pesos. Esto es muy importante definir y depende de la capacidad y memoria de cada computadora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6724 - acc: 0.6081\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6719 - acc: 0.6081\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6721 - acc: 0.6081\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6717 - acc: 0.6081\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6718 - acc: 0.6081\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6723 - acc: 0.6081\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6721 - acc: 0.6081\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6716 - acc: 0.6081\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6717 - acc: 0.6081\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6712 - acc: 0.6081\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6713 - acc: 0.6081\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6711 - acc: 0.6081\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 0s 989us/step - loss: 0.6711 - acc: 0.6081\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6708 - acc: 0.6081\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6709 - acc: 0.6081\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6705 - acc: 0.6081\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 0s 951us/step - loss: 0.6708 - acc: 0.6081\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6706 - acc: 0.6081\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6706 - acc: 0.6081\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6705 - acc: 0.6081\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6703 - acc: 0.6081\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6705 - acc: 0.6081\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6700 - acc: 0.6081\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6707 - acc: 0.6081\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6704 - acc: 0.6081\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6699 - acc: 0.6081\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6698 - acc: 0.6081\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6698 - acc: 0.6081\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6694 - acc: 0.6081\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6692 - acc: 0.6081\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6696 - acc: 0.6081\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6691 - acc: 0.6081\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 0s 996us/step - loss: 0.6690 - acc: 0.6081\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6694 - acc: 0.6081\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6692 - acc: 0.6081\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6697 - acc: 0.6081\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6685 - acc: 0.6081\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 0s 964us/step - loss: 0.6695 - acc: 0.6081\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6692 - acc: 0.6081\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6682 - acc: 0.6081\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6687 - acc: 0.6081\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6687 - acc: 0.6081\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6686 - acc: 0.6081\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6687 - acc: 0.6081\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6682 - acc: 0.6081\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6681 - acc: 0.6081\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6680 - acc: 0.6081\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 0s 977us/step - loss: 0.6687 - acc: 0.6081\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6678 - acc: 0.6081\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 0s 979us/step - loss: 0.6678 - acc: 0.6081\n",
      "Epoch 51/100\n",
      "9/9 [==============================] - 0s 979us/step - loss: 0.6681 - acc: 0.6081\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6681 - acc: 0.6081\n",
      "Epoch 53/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6671 - acc: 0.6081\n",
      "Epoch 54/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6678 - acc: 0.6081\n",
      "Epoch 55/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6678 - acc: 0.6081\n",
      "Epoch 56/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6673 - acc: 0.6081\n",
      "Epoch 57/100\n",
      "9/9 [==============================] - 0s 850us/step - loss: 0.6671 - acc: 0.6081\n",
      "Epoch 58/100\n",
      "9/9 [==============================] - 0s 941us/step - loss: 0.6669 - acc: 0.6081\n",
      "Epoch 59/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6671 - acc: 0.6081\n",
      "Epoch 60/100\n",
      "9/9 [==============================] - 0s 957us/step - loss: 0.6674 - acc: 0.6081\n",
      "Epoch 61/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6670 - acc: 0.6081\n",
      "Epoch 62/100\n",
      "9/9 [==============================] - 0s 814us/step - loss: 0.6667 - acc: 0.6081\n",
      "Epoch 63/100\n",
      "9/9 [==============================] - 0s 888us/step - loss: 0.6671 - acc: 0.6081\n",
      "Epoch 64/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6670 - acc: 0.6081\n",
      "Epoch 65/100\n",
      "9/9 [==============================] - 0s 738us/step - loss: 0.6670 - acc: 0.6081\n",
      "Epoch 66/100\n",
      "9/9 [==============================] - 0s 954us/step - loss: 0.6666 - acc: 0.6081\n",
      "Epoch 67/100\n",
      "9/9 [==============================] - 0s 971us/step - loss: 0.6662 - acc: 0.6081\n",
      "Epoch 68/100\n",
      "9/9 [==============================] - 0s 845us/step - loss: 0.6662 - acc: 0.6081\n",
      "Epoch 69/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6660 - acc: 0.6081\n",
      "Epoch 70/100\n",
      "9/9 [==============================] - 0s 768us/step - loss: 0.6665 - acc: 0.6081\n",
      "Epoch 71/100\n",
      "9/9 [==============================] - 0s 764us/step - loss: 0.6661 - acc: 0.6081\n",
      "Epoch 72/100\n",
      "9/9 [==============================] - 0s 865us/step - loss: 0.6659 - acc: 0.6081\n",
      "Epoch 73/100\n",
      "9/9 [==============================] - 0s 712us/step - loss: 0.6652 - acc: 0.6081\n",
      "Epoch 74/100\n",
      "9/9 [==============================] - 0s 971us/step - loss: 0.6657 - acc: 0.6081\n",
      "Epoch 75/100\n",
      "9/9 [==============================] - 0s 711us/step - loss: 0.6660 - acc: 0.6081\n",
      "Epoch 76/100\n",
      "9/9 [==============================] - 0s 936us/step - loss: 0.6659 - acc: 0.6081\n",
      "Epoch 77/100\n",
      "9/9 [==============================] - 0s 921us/step - loss: 0.6659 - acc: 0.6081\n",
      "Epoch 78/100\n",
      "9/9 [==============================] - 0s 894us/step - loss: 0.6656 - acc: 0.6081\n",
      "Epoch 79/100\n",
      "9/9 [==============================] - 0s 968us/step - loss: 0.6653 - acc: 0.6081\n",
      "Epoch 80/100\n",
      "9/9 [==============================] - 0s 813us/step - loss: 0.6650 - acc: 0.6081\n",
      "Epoch 81/100\n",
      "9/9 [==============================] - 0s 841us/step - loss: 0.6653 - acc: 0.6081\n",
      "Epoch 82/100\n",
      "9/9 [==============================] - 0s 957us/step - loss: 0.6655 - acc: 0.6081\n",
      "Epoch 83/100\n",
      "9/9 [==============================] - 0s 694us/step - loss: 0.6652 - acc: 0.6081\n",
      "Epoch 84/100\n",
      "9/9 [==============================] - 0s 990us/step - loss: 0.6654 - acc: 0.6081\n",
      "Epoch 85/100\n",
      "9/9 [==============================] - 0s 849us/step - loss: 0.6649 - acc: 0.6081\n",
      "Epoch 86/100\n",
      "9/9 [==============================] - 0s 778us/step - loss: 0.6650 - acc: 0.6081\n",
      "Epoch 87/100\n",
      "9/9 [==============================] - 0s 963us/step - loss: 0.6645 - acc: 0.6081\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 819us/step - loss: 0.6643 - acc: 0.6081\n",
      "Epoch 89/100\n",
      "9/9 [==============================] - 0s 882us/step - loss: 0.6644 - acc: 0.6081\n",
      "Epoch 90/100\n",
      "9/9 [==============================] - 0s 923us/step - loss: 0.6642 - acc: 0.6081\n",
      "Epoch 91/100\n",
      "9/9 [==============================] - 0s 779us/step - loss: 0.6642 - acc: 0.6081\n",
      "Epoch 92/100\n",
      "9/9 [==============================] - 0s 909us/step - loss: 0.6639 - acc: 0.6081\n",
      "Epoch 93/100\n",
      "9/9 [==============================] - 0s 830us/step - loss: 0.6647 - acc: 0.6081\n",
      "Epoch 94/100\n",
      "9/9 [==============================] - 0s 729us/step - loss: 0.6645 - acc: 0.6081\n",
      "Epoch 95/100\n",
      "9/9 [==============================] - 0s 900us/step - loss: 0.6643 - acc: 0.6081\n",
      "Epoch 96/100\n",
      "9/9 [==============================] - 0s 718us/step - loss: 0.6643 - acc: 0.6081\n",
      "Epoch 97/100\n",
      "9/9 [==============================] - 0s 966us/step - loss: 0.6638 - acc: 0.6081\n",
      "Epoch 98/100\n",
      "9/9 [==============================] - 0s 817us/step - loss: 0.6637 - acc: 0.6081\n",
      "Epoch 99/100\n",
      "9/9 [==============================] - 0s 878us/step - loss: 0.6631 - acc: 0.6081\n",
      "Epoch 100/100\n",
      "9/9 [==============================] - 0s 929us/step - loss: 0.6630 - acc: 0.6081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14ab94670>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ya entrenamos nuestro modelo, vamos a crear una peque침a funci칩n donde le pasamos algunas variable de entreda como Pclass, Sex, Age, Fare, Emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(Pclass=1, Sex='female', Age=60 ,Fare=0, Embarked='C'):\n",
    "    cnames = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']\n",
    "    data = [[Pclass, Sex, Age, Fare, Embarked]]\n",
    "    my_X = pd.DataFrame(data=data, columns=cnames)\n",
    "    my_X = preprocesador.transform(my_X)\n",
    "    return model.predict_classes(my_X)\n",
    "\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
