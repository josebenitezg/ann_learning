{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Redes Neuronales Artificiales desde cero - Parte 1</h1>\n",
    "\n",
    "En este m√≥dulo aprenderemos los conecptos b√°sicos, las nociones aritm√©ticas y matem√°ticas sobre c√≥mo funciona una red neuronal artificial as√≠ como entener con un c√≥digo simple su funcionamiento y luego utilizar librer√≠as conocidas como TensorFlow 2.0\n",
    "\n",
    "Les recomiendo que agarren unas üç∫ o ‚òï o la bebida que prefieran para que nos embarquemos en esta aventura :P pero antes de empezar me gustar√≠a que empecemos a realizar un peque√±o viaje a la neurociencia üß†...\n",
    "\n",
    "## 1. La Neurona Humana\n",
    "![NeuronUrl](https://media.giphy.com/media/5h9EHCvA0OR2/giphy.gif \"neuron\")\n",
    "\n",
    "En el gif podemos apreciar las redes nueronales y lo incre√≠ble es poder entender c√≥mo imitar el compartamiento de una neurona humana con la de una m√°quina ¬øY por qu√© hacemos esto? es simple, nuestro cerebro humano es una m√°quina compleja capaz de resolver operaciones -que para una computadora parece muy compleja- con much√≠sima facilidad. Imaginate poder recrear, la informaci√≥n que recibimos con nuestros ojos, desde una c√°mara, la percepci√≥n de colores, procesarlos y obtener informaci√≥n para que realice una tarea que nos ayude a mejorar en muchas disciplinas. Imitar el funcionamiento del cerebro humano nos ayuda a extender nuestras posibilidades como seres humanos en una m√°quina.\n",
    "\n",
    "Entonces, nuestro desaf√≠o principal est√° en como recrear una neurona. Para ello, vamos a ver analizar su funcionamiento.\n",
    "\n",
    "![RamonUrl](https://supercurioso.com/wp-content/uploads/2014/12/ramonycajalelartistadelaneurona.jpg \"ramon\")\n",
    "\n",
    "En el a√±o 1899 Santiago Ram√≥n y Cajal (imagen izquierda) lo que hizo fue te√±ir las neuronas y observarlas bajo el microscopio y traz√≥ la imagen de lo que el cre√≠a se trataba de una neurona individual (imagen derecha).\n",
    "\n",
    "A√±os m√°s tarde, con la llegada de nuevas tecnolog√≠as permitieron una mejor observaci√≥n y como mostraremos en la imagen m√°s abajo, una estructura real y gr√°fica de la neurona es muy parecida a la imagen de Santiago.\n",
    "\n",
    "![EstructuraUrl](https://respuestas.tips/wp-content/uploads/2013/10/partes-de-una-neurona.jpg \"estructura\")\n",
    "\n",
    "En realidad, es mucho m√°s compleja en cuanto a todas las partes que la compone, pero quiero que nos enfoquemos en tres partes importantes de la neurona, las dendritas, el ax√≥n y el n√∫cleo.\n",
    "\n",
    "Lo que tenemos que entender aqu√≠ es que las neuronas por s√≠ solas no sirven, deben convivir con otras para su correcto funcionamiento para lograr cosas incre√≠bles que ya conocemos.\n",
    "\n",
    "¬øC√≥mo trabajan juntas?\n",
    "\n",
    "Bueno, las dendritas, son las encargadas de recibir la informaci√≥n, luego pasan por el n√∫cleo y el ax√≥n se encarga de llevar esta informaci√≥n a otra neurona, b√°sicamente transmiten impulsos el√©ctricos desde el ax√≥n a las dendritas de otra neurona y este proceso es al que se le conoce como sinapsis y para que se hagan una idea nuestra cerebro en edad adulta cuenta con 86 mil millones de neuronas en promedio.\n",
    "\n",
    "Ahora, vamos a empezar a estudiar como logramos plasmar la neurona humana en una m√°quina.\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/642/1*ifwiDM2fRKze9VBG2p6_Bg.png \"estructura\")\n",
    "\n",
    "Entre las d√©cadas del 1950 y 1960 el psicol√≥go y cient√≠fico estadounidense Frank Rosenblatt, inspirado en el trabajo de otros cient√≠ficos, crea algo que hoy conocemos como perceptr√≥n (imagen de arriba). Que es la √∫nidad b√°sica de inferencia o una neurona artificial.\n",
    "\n",
    "Ahora imaginemos a la puntas de las felchas como dendritas y las *X* como informaci√≥n que le pasa otra neurona, entonces las flechas ser√≠an las sinapsis. En este caso la se√±al que le pasamos, vendr√≠a a ser una valor de entrada y a partir de estos valores generan un valor de salida. A las entradas les llamamos *variables independientes* porque pueden ser cualquier valor por separado y a la salida como *variable dependiente* porque es un resultado que depende de las entradas. Y a las la sinapsis que vemos ahora los llamaremos pesos *w* como la imagen de abajo:\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/642/1*ifwiDM2fRKze9VBG2p6_Bg.png \"estructura\")\n",
    "\n",
    "Esta sinapsis que ahora le llamamos pesos o com√∫nmente representado por *w* son fundamentales para el correcto funcionamiento de nuestro perceptr√≥n porque estos valores de pesos se utilizan para que pueda aprender qu√© valor de entrada a la neuora es importante y que no. De hecho lo √∫nico que ajustamos cuando entrenamos una red neuronal es ajustar los valores de los pesos constantemente, pero ¬øpor qu√©?\n",
    "\n",
    "El entrenamiento tiene dos pasos\n",
    "\n",
    "   - Primer paso:\n",
    "    El entrenamiento lo que hace es una suma total del valor de entrada multiplicado por los pesos es decir: x1.w1 + x2.w2 + xn*wn. A esto se le conoce como suma ponderada y podemos representarlo con la siguiente ecuaci√≥n:\n",
    "    \n",
    "   $$\\sum\\limits_{i=0}^{m} {x_i*w_i}$$  \n",
    "    \n",
    "   - Segundo paso:\n",
    "    A nuestra suma ponderada le aplicamos una funci√≥n de activaci√≥n delta (no se preocupen veremos a continuaci√≥n que es esta funci√≥n). Quedando as√≠ finalmente \n",
    "    \n",
    "   $$\\delta(\\sum\\limits_{i=0}^{m} {x_i*w_i})$$ \n",
    "\n",
    "\n",
    "## 2. Funci√≥n de activaci√≥n\n",
    "\n",
    "La funci√≥n de activaci√≥n es la que me permite definir el comportamiento de mi neurona y de cada capa, esta funci√≥n decide que valor pasa o se queda. Hablaremos brevemente de 4 de ellas.\n",
    "\n",
    "- Threshold Function o escal√≥n unitario\n",
    "![EstructuraUrl](https://miro.medium.com/max/960/0*etGrZj_m0spvgN9n.png \"estructura\")\n",
    "Esta es la funci√≥n m√°s simple, como ver√°n cuando recibe valore negativos (menores a 0) la funci√≥n est√° en cero y cuando recibe valores positivos (mayores a 0) la funci√≥n pasa a ser 1, es decir nuestro valor de salida siempre va a ser 1 y 0 nada m√°s. Ahora que comprendemos como es una funci√≥n de activaci√≥n b√°sica vamos a uno un poco m√°s compleja.\n",
    "\n",
    "\n",
    "- Funci√≥n Sigmoide\n",
    "![EstructuraUrl](https://miro.medium.com/max/744/1*zSDNMn4z_hmYing0wDCSoQ.png \"sigmoide\")\n",
    "Como vemos esta funci√≥n es un poco m√°s suave, pasa por todos los valores entre 0 y 1, es muy √∫ltil cuando queremos calcular probabilidades en nuestra salida.\n",
    "\n",
    "\n",
    "- Funci√≥n Rectificadora\n",
    "![EstructuraUrl](https://miro.medium.com/max/736/1*RthaUGkni6YCtwlAKsadsg.png \"recti\")\n",
    "Es una de las funciones m√°s utilizadas en Machine Learning, esto lo que hace simplemente es entregarnos los valores cuando se√°n mayores a cero, es decir, valor positivo. Para cada valor positivo un valor de la curva.\n",
    "\n",
    "\n",
    "- Tangente Hiperb√≥lica\n",
    "![EstructuraUrl](https://miro.medium.com/max/772/1*sYnLa9djWZ7BozqUV5lNmw.png \"hiper\")\n",
    "Esta funci√≥n es muy similar a la Sigmoide pero va desde -1 a 1 los valores.\n",
    "\n",
    "Quisiera que puedas tener en mente esto ya que todav√≠a no entraremos a profundidad pero si los veremos m√°s adelante.\n",
    "\n",
    "## 3. ¬øC√≥mo funcionan las Redes Neuronales Artificiales en un caso real?\n",
    "\n",
    "Vamos a imaginar que hacemos una clasificaci√≥n, vamos a usar un dataset, en este caso y para ejemplo el de Titanic, donde nos trae una lista de pasajeros y si sobrevivieron o no. A continuaci√≥n vamos a representar nuestro perceptr√≥n con las variables y se ve as√≠:\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/1084/1*_pD95maLHcEL8Hc39EMJGA.png \"ti\")\n",
    "\n",
    "Para este caso es s√∫per f√°cil calcular ya que el resultado es simplemente la suma ponderada con la funci√≥n de activaci√≥n. Pero que pasa si a√±ado m√°s capacas de a nuestro modelo, es decir, todas las entradas iran conectadas a la capa siguiente como vemos en la siguiente imagen, eso significa que podr√≠amos predecir m√°s valores, aunque no piensen que si tenemos m√°s capa mejora el resultado.\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/1084/1*YjtSTddpDPCscJSXcQblBg.png \"j\")\n",
    "\n",
    "Mientras m√°s capas tengas m√°s nivel de abstracci√≥n va a tener nuestra red. Recuerden que b√°sicamente cada una recibe un valor de peso distinto por lo que puede o no recibir esa entrada \"x\" en algunos perceptrones, de esta manera, cada uno realiza una funci√≥n difrente, aunque no siempre sea as√≠.\n",
    "\n",
    "## 4. ¬øC√≥mo aprenden las redes neuronales?\n",
    "\n",
    "En programaci√≥n hay muchas maneras de llegar a una soluci√≥n, por ejemplo, podr√≠amos programar todos los casos y respuestas para una soluci√≥n posible. Cuando esta tarea lleva variables que no siempre se van a comportar como el algoritmo espera, por ejemplo. Queremos crear algo que reconzca un perro en cualquier fotos sin importar la raza. Suena sencillo, clasifico las imagenes, digo que son perros, y listo. Pero esta tarea se complica ya que existen *N* razas de perros, pueden salir en *N* tipo de ambientes en la fotograf√≠a y con diferentes tipos de poses, para esto, es mucho m√°s complicado tener un c√≥digo que lo resuleva o programar cada caso, para ello se crean las redes neuronales artificiales, claro, para nosotros es natural saber que hay un perro en la foto pero para la m√°quina no, es esa capacidad humana justamente el que estamos tratando de imitar.\n",
    "\n",
    "La ventaja de las redes neuronales es que puede \"entender\" los datos que le damos de entrada y hacer una funci√≥n para cualquier entrada que le presentemos, y como buen ejemplo hemos dado la imagen del perro.\n",
    "\n",
    "Veamos de nuevo la red neuronal anterior...\n",
    "![EstructuraUrl](https://miro.medium.com/max/1084/1*YjtSTddpDPCscJSXcQblBg.png \"j\")\n",
    "Lo que vamos a hacer es calcular el valor de la salida *Y* (si sobrevivi√≥ o no al Titanic) en base a nuestro dato de entrada *X* (variables que vemos arriba). Adem√°s de esto le vamos a pasar a *Y* valores reales del dataset de Titanic (si sobrevivi√≥ o no realmente). ¬øPero entonces de que me sirve la predicci√≥n en *Y* si le paso los valores reales?. Es simple, al final de cada paso por la neurona, la m√°quina lo que va a a hacer es comparar el valor de salida de la predicci√≥n con el valor real, esto se hace a trav√©s de la funci√≥n de p√©rdida y podemos verlo en la siguiente ecuaci√≥n:\n",
    "$$L = {1 \\over 2}(\\Upsilon - y)^2$$\n",
    "Aqu√≠ Upsilon (la y medio rara que se ve ah√≠ ü§£) es nuestro valor predecido y la *Y* normal es nuestro valor real. Esta es la m√°s com√∫n entre las funciones de p√©rdida y esta se llama \"Mean Squered Error\" o la medidad de los errores al cuadrado. Esta funci√≥n lo que nos dice es cu√°nto es el error que va a tener nuestra red una vez que tenemos la entrada y se realiza la suma ponderada, lo que tenemos que hacer es minimizarla.\n",
    "Esta funci√≥n luego va para atr√°s y corrije los pesos para ir minizando los pesos \"w\", de esto hablaremos en un rato.\n",
    "\n",
    "## 5. ¬øC√≥mo es que minimizo mi funci√≥n de perdida?\n",
    "\n",
    "Bueno para poder minizar la funci√≥n de perdida podr√≠amos utilizar algo que se llama el descenso de gradiente (aclaro que este texto no muestra la unica forma pero si trato de simplificar lo m√°ximo posible para adentrarnos en este complejo mundo).\n",
    "Lo que hace esta funci√≥n es tratar de encontrar los valores de los pesos de modo a que el error se ajuste con la pendiente de la recta (derivada de la funci√≥n) dentro del conjunto de valores (puntos azules) que ser√≠an las entradas como se aprecia en el siguiente gif:\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/proxy/1*KQVi812_aERFRolz_5G3rA.gif)\n",
    "\n",
    "Pero que pasa si nuestra funci√≥n no representa una parabola y tiene varios m√≠nimos como se ve a continuaci√≥n\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/1400/0*XKMTnu8MwD2wdso6.png)\n",
    "\n",
    "Si nuestro algoritmo solamente encuentra un m√≠nimo local no encontrar√° un m√≠nimo global que es el que neceitamos para que se ajusten los valores de los pesos.\n",
    "\n",
    "Entonces con el descendo de gradiente lo que nuestra funci√≥n hace en cada paso es:\n",
    "- Paso el dato 1\n",
    "- Paso el dato 2\n",
    "- Paso el dato 3\n",
    "- Ajusto los pesos\n",
    "\n",
    "Para solucionar el problema anterior donde tenemos varios m√≠nimos posibles se utiliza otro algoritmo que hace algo que se llama descenso de gradiente estoc√°stico y hace lo siguiente\n",
    "- Paso el dato 1\n",
    "- Ajusto los pesos\n",
    "- Paso el dato 2\n",
    "- Ajusto los pesos\n",
    "- Paso el dato 3\n",
    "- Ajusto los pesos\n",
    "\n",
    "Entonces cada vez que pase por el dato va a justar los pesos y se acercar√° m√°s al m√≠nimo global ya que se mueve constantemente ante cada paso de datos.\n",
    "\n",
    "## 5. Backpropagation\n",
    "\n",
    "Todo lo que vimos hasta ahora es lo que se llama comunmente Forward Propagation, donde el valor de entrada, pasa por toda la red, pasa por la funci√≥n de activaci√≥n, se calcula la funci√≥n de p√©rdida, luego de este proceso la p√©rdida se debe \"propagar hacia atr√°s\" ajustando los valores de los pesos, esto es lo que hace el backpropagation, ir para atr√°s y ajustar los valores de los pesos a la vez en simult√°neo, es un algoritmo con unas matem√°ticas muy avanzadas e interesantes que los puedes mirar por tu cuenta si quieres :)\n",
    "\n",
    "## 6. Manos a la obra\n",
    "\n",
    "Ahora lo que haremos es aplicar una peque√±a red neuronal sin utilizar ninguna librer√≠a especial excepto la de numpy. Numpy es una liber√≠a de python que nos permite realizar operaciones matem√°ticas complejas con una f√°cil implementaci√≥n del c√≥digo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Como primer paso lo que haremos es instalar esta liber√≠a en caso de que nuestra PC no lo tenga, si est√°s usando alg√∫n otro editor puedes instalarlo desde tu terminal quitando el signo de exclamaci√≥n que tiene en frente.\n",
    "\n",
    "Recuerden que para ejecutar un bloque de c√≥digo como el que ven abajo solo deben ubicar el cursos sobre el bloque y presionar SHIFT+ENTER y as√≠ sucesivamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.18.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Importamos la librer√≠a de numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lo que hacemos en este punto crear una funci√≥n que calcule nuestra funci√≥n de activaci√≥n, que ser√≠a una variante de la funci√≥n sigmoide.\n",
    "Tengamos en cuenta que creamos esta funci√≥n para usarlo en sus dos formas, si queremos como sus derivadas o no.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlin(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lo que hacemos aqu√≠ abajo es crear dos matrices, una matriz \"x\" que ya sabemos que es la entrada \"y\" que son los valores reales de la salida osea los resultados reales.\n",
    "\n",
    "Recuerden que esos datos que presentamos en la entrada son valores cualquiera, solo representan una entrada que queremos que nos brinde una salida predictiva.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,0,1],\n",
    "      [0,1,1],\n",
    "      [1,0,1],\n",
    "      [1,1,1]])\n",
    "y = np.array([[0],\n",
    "      [1],\n",
    "      [1],\n",
    "      [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Aqu√≠ lo que haremos es crear valores de pesos de manera randomica, que la computadora me los genere por que es lo que voy a ir corrigiendo con el algoritmo. Los imprimo para que los veamos.\n",
    "Supongamos que vamos a tener dos capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
      " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n",
      " [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n",
      "[[-0.5910955 ]\n",
      " [ 0.75623487]\n",
      " [-0.94522481]\n",
      " [ 0.34093502]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "w1 = 2*np.random.random((3,4))-1\n",
    "w2 = 2*np.random.random((4,1))-1\n",
    "\n",
    "print(w1)\n",
    "print(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bueno aqu√≠ empieza lo divertido üòä\n",
    "Supogamos que L1 y L2 son las capas.\n",
    "Lo que vamos a hacer primero es el #Forward Propagation, en este paso primero guardamos en \"L0\" nuestros valores de \"x\" y luego hacemos un producto punto con los valores de los pesos (suma ponderarda) y lo pasamos por la funci√≥n de activaci√≥n como se puestra en L1, estos valores se multiplican con los valores anteriores y los pesos de la capa 2.\n",
    "\n",
    "Luego en #C√°lculo de erro lo que hacemos es calcular nuestro error de los valores reales con nuestras predicciones para hallar el valor de p√©rdida. Esto hacemos restando el valor de real con el valor que se predijo.\n",
    "\n",
    "¬øRecuerdan que hablamos sobre el algoritmo de descenso de gradiente?. Bueno eso se calcula multiplicando el error actual por la derivada de la funci√≥n que creamos con las predicciones actuales (por eso decidimos tener una opci√≥n de derivada m√°s arriba) Recuerde la derivada es la recta tangente en un punto a la funci√≥n.\n",
    "\n",
    "#Backpropagation.. Una vez que tengamos el descenso de gradiente (la pendiente) multimplicamos por la transpuesta de los pesos de la capa siguiente y de este modo propagaremos el error hacia atr√°s.\n",
    "\n",
    "Y finalmente #ajustamos el valor de los pesos con la transpuesta de las capas por sus respectivas optimizaciones o direcciones hacia donde actualizar los pesos y esto suman a nuestros pesos actuales modificando los dos a la vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(6000):\n",
    "    #Forward Propagation\n",
    "    L0 = x\n",
    "    L1 = nonlin(np.dot(L0,w1))\n",
    "    L2 = nonlin(np.dot(L1,w2)) \n",
    "    \n",
    "    #C√°lculo de error\n",
    "    L2_error = y - L2       \n",
    "    \n",
    "    #Calculamos el descenso de gradiente\n",
    "    L2_delta = L2_error*nonlin(L2,deriv=True)\n",
    "    \n",
    "    #Backpropagation\n",
    "    L1_error = L2_delta.dot(w2.T)\n",
    "    L1_delta = L1_error * nonlin(L1,deriv=True)\n",
    "    \n",
    "    #ajustamos el valor de los pesos\n",
    "    w2 += L1.T.dot(L2_delta)\n",
    "    w1 += L0.T.dot(L1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora analicemos los resultados, vemos como aqu√≠ nuestro valores \"y\" que son los reales vs los valores que la m√°quina predijo que ser√≠an. ¬øMuy loco no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores reales\n",
      "\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "\n",
      "Valores de las predicciones\n",
      "\n",
      "[[0.00631758]\n",
      " [0.99192018]\n",
      " [0.99291766]\n",
      " [0.0094266 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Valores reales\\n\")\n",
    "print(y)\n",
    "print(\"\\n\")\n",
    "print(\"Valores de las predicciones\\n\")\n",
    "print(L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Ahora s√≠, el dataset de Titanic con Tensorflow 2.0 Keras API\n",
    "\n",
    "![EstructuraUrl](https://cdn-images-1.medium.com/fit/t/1600/480/1*cozITwlOo6tzXSWrSxv_Cg.jpeg)\n",
    "\n",
    "Vamos a implementar el modelo de deep learning s√∫per basico en la librer√≠a Tensorflow con API de Keras\n",
    "\n",
    "Primero vamos a correr los siguientes comandos en la consola:\n",
    "\n",
    "Primero vamos a correr los siguientes comandos en nuestra consola\n",
    "```python\n",
    "pip3 install scikit-learn\n",
    "pip3 install pandas\n",
    "pip3 install numpy\n",
    "pip3 install tensorflow\n",
    "```\n",
    "En mi caso lo corro aqui en jupyter notebook\n",
    "\n",
    "El dataset (un conjunto de datos) del Titanic que hablamos inicialmente lo descargamos de [AQU√ç](https://www.kaggle.com/c/titanic). Deber√≠an Registrarse a KAGGLE es sencillo y luego entrar en la secci√≥n data y descargar todos los datos.\n",
    "Una vez descargado, descomprime los archivos, dentro de la carpeta donde tengas tu c√≥digo.\n",
    "Ahora vamos a codificar paso a paso.\n",
    "Vale aclarar que el csv es una archivo que contiene textos o valores separados por comas, comma separated values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Vamos a empezar importando las librer√≠as que vamos a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Ahora vamos a cargar el dataset que contiene nuestro .csv espec√≠ficamente el archivo train.csv para ello utilizamos pandas. Esto nos permitir√° adem√°s poder ver que variables no son importantes para nuestro entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/josebenitez/Documents/test_ANN/titanic/train.csv')\n",
    "df.head() #esta funci√≥n head nos permite mostrar los primeros 5 datos de nuestro datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqu√≠ solo con la vista podemos descartar muchas variables que son las siguientes\n",
    "- Nombre: No nos sirve porque no aporta informaci√≥n al resultado\n",
    "- Ticket: Tampoco nos aporta Informaci√≥n\n",
    "- Cabin: Podr√≠a aportarnos informaci√≥n pero tiene valores perdidos\n",
    "- PassangerID: Es el indice, eso no aporta ni una informaci√≥n relevante\n",
    "- Parch: Lo eliminaremos para tener una peor rendimiento para saber como optimizar el modelo en la siguiente secci√≥n\n",
    "\n",
    "Entonces los datos de entrada X van a tener los siguientes valores:\n",
    "- Sex\n",
    "- Age\n",
    "- SibSp: cantidad de hermanos o esposa\n",
    "- Fare: impuesto que pagan\n",
    "- Embarked: es la clase donde embarc√≥\n",
    "\n",
    "Con eso ya podemos separa la nuestra variable independiente de la dependiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Pclass', 'Sex', 'Age', 'Embarked', 'Fare', 'SibSp'])\n",
    "\n",
    "X = df[['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']]\n",
    "y = df.Survived.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lo que vamos a ver continuaci√≥n, un preprocesado de los datos, es tal vez un poco abstracto. Es decir, cuesta un poco entender r√°pidamente pero voy a tratar de simplificarlo todo lo que pueda. Bueno, como vieron arriba tenemos varios tipos de datos que tienen un rango muy alto o muy bajo entre uno y otro por ejemplo el de las edades, porque al pasar por mi funci√≥n de activaci√≥n, √©sta no nos entregar√° resutados √≥ptimos ya que va a tener muchas fluctuaciones. Lo que haremos es normalizar estos datos, que queden en una escala donde no tengan mucha diferencia entre valores.\n",
    "\n",
    "Entonces lo que hacemos es separar los valores primero, sabemos que Sex, Pclass y Embarked son valores que pertenecen a categor√≠as, es decir, para las matem√°ticas no es m√°s ni menos los que est√°n en la primera clase que en la segunda.\n",
    "Pero en el caso de Age y Fare no podemos darle una categor√≠a directamente porque son valores que var√≠an, para ello vamos a utilizar la funci√≥n StandardScaler para poder tenerlos en una misma escala como lo mencion√© m√°s arriba. Les recomiendo que estudien estas librer√≠as ya que no quiero desviar el tema que estamos tratando.\n",
    "\n",
    "Les dejo igual una explicaci√≥n que encontr√© en StackOverflow y me gust√≥ mucho:\n",
    ">La idea detr√°s de StandardScaler es que transformar√° sus datos de manera que su distribuci√≥n tenga un valor medio 0 y una desviaci√≥n est√°ndar de 1.\n",
    "En el caso de datos multivariados, esto se hace en funci√≥n de las caracter√≠sticas (en otras palabras, de forma independiente para cada columna de datos).\n",
    "Dada la distribuci√≥n de los datos, cada valor en el conjunto de datos tendr√° el valor medio restado, y luego dividido por la desviaci√≥n est√°ndar de todo el conjunto de datos (o caracter√≠stica en el caso multivariante).\n",
    "\n",
    "Marea un poco la expliaci√≥n f√°cil :P . Pero no se preocupen, leyendo y releyendo se termina aprendiendo. Me pas√≥ lo mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocesador = make_column_transformer(\n",
    "    (StandardScaler(), ['Age','Fare']),\n",
    "    (OneHotEncoder(), ['Sex','Pclass','Embarked'])\n",
    ")\n",
    "\n",
    "X = preprocesador.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Ahora armaremos el perceptron, para ello importamos las neuronas simples y el modo secuencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a construir el modelo inicial, la primera capa Dense recibe el n√∫mero de datos por fila y como activaci√≥n utilzaremos la funci√≥n rectificadora. La capa de salida debe tener la misma dimensi√≥n como cantidad de salidas necesito. En este caso solo tengo 1 o 0 como salida por lo que aplico la funci√≥n sigmoide en la salida para calcular la probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a compilar y que el error lo trate como una salida binaria, el optimizador ser√° nuestra funci√≥n derivada que nos permite saber hacia donde mover los pesos. Vamos a realizar 100 pasos y el batch_size nos dice cuantas veces vamos a pasar por los datos para actualizar los pesos. Esto es muy importante definir y depende de la capacidad y memoria de cada computadora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6724 - acc: 0.6081\n",
      "Epoch 2/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6719 - acc: 0.6081\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6721 - acc: 0.6081\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6717 - acc: 0.6081\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6718 - acc: 0.6081\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6723 - acc: 0.6081\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 0.6721 - acc: 0.6081\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6716 - acc: 0.6081\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6717 - acc: 0.6081\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6712 - acc: 0.6081\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6713 - acc: 0.6081\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6711 - acc: 0.6081\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 0s 989us/step - loss: 0.6711 - acc: 0.6081\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6708 - acc: 0.6081\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6709 - acc: 0.6081\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6705 - acc: 0.6081\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 0s 951us/step - loss: 0.6708 - acc: 0.6081\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6706 - acc: 0.6081\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6706 - acc: 0.6081\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6705 - acc: 0.6081\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6703 - acc: 0.6081\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6705 - acc: 0.6081\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6700 - acc: 0.6081\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6707 - acc: 0.6081\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6704 - acc: 0.6081\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6699 - acc: 0.6081\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6698 - acc: 0.6081\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6698 - acc: 0.6081\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6694 - acc: 0.6081\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6692 - acc: 0.6081\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6696 - acc: 0.6081\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6691 - acc: 0.6081\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 0s 996us/step - loss: 0.6690 - acc: 0.6081\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6694 - acc: 0.6081\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6692 - acc: 0.6081\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6697 - acc: 0.6081\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6685 - acc: 0.6081\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 0s 964us/step - loss: 0.6695 - acc: 0.6081\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6692 - acc: 0.6081\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6682 - acc: 0.6081\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6687 - acc: 0.6081\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6687 - acc: 0.6081\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6686 - acc: 0.6081\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6687 - acc: 0.6081\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6682 - acc: 0.6081\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6681 - acc: 0.6081\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6680 - acc: 0.6081\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 0s 977us/step - loss: 0.6687 - acc: 0.6081\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6678 - acc: 0.6081\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 0s 979us/step - loss: 0.6678 - acc: 0.6081\n",
      "Epoch 51/100\n",
      "9/9 [==============================] - 0s 979us/step - loss: 0.6681 - acc: 0.6081\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6681 - acc: 0.6081\n",
      "Epoch 53/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6671 - acc: 0.6081\n",
      "Epoch 54/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6678 - acc: 0.6081\n",
      "Epoch 55/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6678 - acc: 0.6081\n",
      "Epoch 56/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6673 - acc: 0.6081\n",
      "Epoch 57/100\n",
      "9/9 [==============================] - 0s 850us/step - loss: 0.6671 - acc: 0.6081\n",
      "Epoch 58/100\n",
      "9/9 [==============================] - 0s 941us/step - loss: 0.6669 - acc: 0.6081\n",
      "Epoch 59/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6671 - acc: 0.6081\n",
      "Epoch 60/100\n",
      "9/9 [==============================] - 0s 957us/step - loss: 0.6674 - acc: 0.6081\n",
      "Epoch 61/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6670 - acc: 0.6081\n",
      "Epoch 62/100\n",
      "9/9 [==============================] - 0s 814us/step - loss: 0.6667 - acc: 0.6081\n",
      "Epoch 63/100\n",
      "9/9 [==============================] - 0s 888us/step - loss: 0.6671 - acc: 0.6081\n",
      "Epoch 64/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6670 - acc: 0.6081\n",
      "Epoch 65/100\n",
      "9/9 [==============================] - 0s 738us/step - loss: 0.6670 - acc: 0.6081\n",
      "Epoch 66/100\n",
      "9/9 [==============================] - 0s 954us/step - loss: 0.6666 - acc: 0.6081\n",
      "Epoch 67/100\n",
      "9/9 [==============================] - 0s 971us/step - loss: 0.6662 - acc: 0.6081\n",
      "Epoch 68/100\n",
      "9/9 [==============================] - 0s 845us/step - loss: 0.6662 - acc: 0.6081\n",
      "Epoch 69/100\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.6660 - acc: 0.6081\n",
      "Epoch 70/100\n",
      "9/9 [==============================] - 0s 768us/step - loss: 0.6665 - acc: 0.6081\n",
      "Epoch 71/100\n",
      "9/9 [==============================] - 0s 764us/step - loss: 0.6661 - acc: 0.6081\n",
      "Epoch 72/100\n",
      "9/9 [==============================] - 0s 865us/step - loss: 0.6659 - acc: 0.6081\n",
      "Epoch 73/100\n",
      "9/9 [==============================] - 0s 712us/step - loss: 0.6652 - acc: 0.6081\n",
      "Epoch 74/100\n",
      "9/9 [==============================] - 0s 971us/step - loss: 0.6657 - acc: 0.6081\n",
      "Epoch 75/100\n",
      "9/9 [==============================] - 0s 711us/step - loss: 0.6660 - acc: 0.6081\n",
      "Epoch 76/100\n",
      "9/9 [==============================] - 0s 936us/step - loss: 0.6659 - acc: 0.6081\n",
      "Epoch 77/100\n",
      "9/9 [==============================] - 0s 921us/step - loss: 0.6659 - acc: 0.6081\n",
      "Epoch 78/100\n",
      "9/9 [==============================] - 0s 894us/step - loss: 0.6656 - acc: 0.6081\n",
      "Epoch 79/100\n",
      "9/9 [==============================] - 0s 968us/step - loss: 0.6653 - acc: 0.6081\n",
      "Epoch 80/100\n",
      "9/9 [==============================] - 0s 813us/step - loss: 0.6650 - acc: 0.6081\n",
      "Epoch 81/100\n",
      "9/9 [==============================] - 0s 841us/step - loss: 0.6653 - acc: 0.6081\n",
      "Epoch 82/100\n",
      "9/9 [==============================] - 0s 957us/step - loss: 0.6655 - acc: 0.6081\n",
      "Epoch 83/100\n",
      "9/9 [==============================] - 0s 694us/step - loss: 0.6652 - acc: 0.6081\n",
      "Epoch 84/100\n",
      "9/9 [==============================] - 0s 990us/step - loss: 0.6654 - acc: 0.6081\n",
      "Epoch 85/100\n",
      "9/9 [==============================] - 0s 849us/step - loss: 0.6649 - acc: 0.6081\n",
      "Epoch 86/100\n",
      "9/9 [==============================] - 0s 778us/step - loss: 0.6650 - acc: 0.6081\n",
      "Epoch 87/100\n",
      "9/9 [==============================] - 0s 963us/step - loss: 0.6645 - acc: 0.6081\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 819us/step - loss: 0.6643 - acc: 0.6081\n",
      "Epoch 89/100\n",
      "9/9 [==============================] - 0s 882us/step - loss: 0.6644 - acc: 0.6081\n",
      "Epoch 90/100\n",
      "9/9 [==============================] - 0s 923us/step - loss: 0.6642 - acc: 0.6081\n",
      "Epoch 91/100\n",
      "9/9 [==============================] - 0s 779us/step - loss: 0.6642 - acc: 0.6081\n",
      "Epoch 92/100\n",
      "9/9 [==============================] - 0s 909us/step - loss: 0.6639 - acc: 0.6081\n",
      "Epoch 93/100\n",
      "9/9 [==============================] - 0s 830us/step - loss: 0.6647 - acc: 0.6081\n",
      "Epoch 94/100\n",
      "9/9 [==============================] - 0s 729us/step - loss: 0.6645 - acc: 0.6081\n",
      "Epoch 95/100\n",
      "9/9 [==============================] - 0s 900us/step - loss: 0.6643 - acc: 0.6081\n",
      "Epoch 96/100\n",
      "9/9 [==============================] - 0s 718us/step - loss: 0.6643 - acc: 0.6081\n",
      "Epoch 97/100\n",
      "9/9 [==============================] - 0s 966us/step - loss: 0.6638 - acc: 0.6081\n",
      "Epoch 98/100\n",
      "9/9 [==============================] - 0s 817us/step - loss: 0.6637 - acc: 0.6081\n",
      "Epoch 99/100\n",
      "9/9 [==============================] - 0s 878us/step - loss: 0.6631 - acc: 0.6081\n",
      "Epoch 100/100\n",
      "9/9 [==============================] - 0s 929us/step - loss: 0.6630 - acc: 0.6081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14ab94670>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Ahora que ya entrenamos nuestro modelo, vamos a crear una peque√±a funci√≥n donde le pasamos algunas variable de entreda como Pclass, Sex, Age, Fare, Embarked y que nos arroje los resultados de si sobrevivi√≥ o no.\n",
    "\n",
    "Podr√≠amos jugar con esos valores cambi√°ndolos y probando para poder ver como se comporta nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]], dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(Pclass=1, Sex='female', Age=60 ,Fare=0, Embarked='C'):\n",
    "    cnames = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']\n",
    "    data = [[Pclass, Sex, Age, Fare, Embarked]]\n",
    "    my_X = pd.DataFrame(data=data, columns=cnames)\n",
    "    my_X = preprocesador.transform(my_X)\n",
    "    return model.predict_classes(my_X)\n",
    "\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
