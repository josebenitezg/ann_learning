{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Plasmando en código el funcionamiento de las redes neuronales Artificiales - Parte 1</h1>\n",
    "\n",
    "### Para que podamos entrar en contexto sobre nuestro tema principal, primero les quiero hacer hacer una pequeña explicación de cómo funciona una neurona simple en el ser humano, cuyo compratmiento es el que queremos imitar en una máquina.\n",
    "\n",
    "\n",
    "En este módulo aprenderemos los conecptos básicos y las noticiones aritméticas y estadistas de cómo funciona una red neuronal artificial así como entener con un código simple su funcionamiento y luego utilizar librerías conocidas como TensorFlow 2.0\n",
    "\n",
    "## 1. La Neurona Humana\n",
    "![NeuronUrl](https://media.giphy.com/media/5h9EHCvA0OR2/giphy.gif \"neuron\")\n",
    "\n",
    "En el gif podemos apreciar las redes nueronales y lo increíble de esto es poder entender cómo imitar el compartamiento de una neurona humana con la de una máquina ¿Y por qué hacemos esto? es simple, nuestro cerebro humano es una máquina compleja capaz de resolver operaciones -que para una computadora parece muy compleja- con muchísima facilidad. Imaginate poder recrear, la información que recibimos con nuestros ojos desde una cámara, la percepción de colores, procesarlos y obtener información para que realice una tarea que nos ayude. Imitar el funcionamiento del cerebro humano nos ayuda a extender nuestras posibilidades como seres humanos.\n",
    "\n",
    "Entonces nuestro desafío principal está en como recrear una neurona. Para ello, vamos a ver analizar su funcionamiento.\n",
    "\n",
    "![RamonUrl](https://supercurioso.com/wp-content/uploads/2014/12/ramonycajalelartistadelaneurona.jpg \"ramon\")\n",
    "\n",
    "En el año 1899 Santiago Ramón y Cajal (imagen derecha) lo que hizo fue teñir las neuronas y observarlas bajo el microscopio y trazó la imagen de lo que el creía se trataba de una neurona individual (imagen izquierda).\n",
    "\n",
    "Años más tarde, con la llegada de nuevas tecnologías permitieron una mejor observación y como mostraremos en la imagen más abajo, una estructura real y gráfica de la neurona es muy parecida a la imagen de Santiago.\n",
    "\n",
    "![EstructuraUrl](https://respuestas.tips/wp-content/uploads/2013/10/partes-de-una-neurona.jpg \"estructura\")\n",
    "\n",
    "En realidad, es mucho más compleja en cuanto a todas las partes que la componen, pero quiero que nos enfoquemos en tres partes importantes de la neurona, las dendritas, el axón y el núcleo.\n",
    "\n",
    "Lo que tenemos que entender aquí es que las neuronas por sí solas no sirven, deben convivir con otras para su correcto funcionamiento para lograr cosas increíbles que ya conocemos.\n",
    "\n",
    "¿Cómo trabajan juntas?\n",
    "\n",
    "Bueno, las dendritas, reciben la información, pasa por el núcleo y el axón se encarga de llevar esta información a otra neurona, básicamente transmiten impulsos eléctricos desde el axón a las dendritas de otra neurona y este proceso es al que se le conoce como sinapsis y para que se hagan una idea nuestra cerebro en edad adulta cuenta con 86 mil millones de neuronas en promedio.\n",
    "\n",
    "Ahora, vamos a empezar a estudiar como logramos plasmar la neurona humana en una máquina.\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/642/1*ifwiDM2fRKze9VBG2p6_Bg.png \"estructura\")\n",
    "\n",
    "Entre las décadas del 1950 y 1960 el psicológo y científico estadounidense, inspirado en el trabajo de otros científicos, crea algo que hoy conocemos como perceptrón (imagen de arriba). Que es la únidad básica de inferencia o una neurona artificial.\n",
    "\n",
    "Ahora imaginemos a las fechas como dendritas y las X como información que le pasa otra neurona, entonces las flechas serían las sinapsis. En este caso la señal que le pasamos, vendría a ser una valor de entrada y a partir de estos valores generan un valor de salida. A las entradas les llamamos variables independientes porque pueden ser cualquier valor por separado y a la salida como variable dependiente porque es un resultado que depende de las entradas. Y a las la sinapsis que vemos ahora los llamaremos pesos \"w\" como la imagen de abajo:\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/642/1*ifwiDM2fRKze9VBG2p6_Bg.png \"estructura\")\n",
    "\n",
    "Esta sinapsis que ahora le llamaremos pesos o comúnmente representado por \"w\" son fundamentales para el correcto funcionamiento de nuestro perceptrón porque estos valores de pesos se utilizan para que pueda aprender qué valor de entrada a la neuora es importante y que no. De hecho lo único que ajustamos cuando entrenamos una red neuronal es ajustar los valores de los pesos constantemente, pero ¿por qué?\n",
    "\n",
    "El entrenamiento tiene dos pasos\n",
    "\n",
    "    Primer paso:\n",
    "    El entrenamiento lo que hace es una suma total del valor de entrada multiplicado por los pesos es decir: x1.w1 + x2.w2 + xn*wn. A esto se le conoce como suma ponderada y podemos representarlo con la siguiente ecuación:\n",
    "    \n",
    "   $$\\sum\\limits_{i=0}^{m} {x_i*w_i}$$  \n",
    "    \n",
    "    Segundo paso:\n",
    "    A nuestra suma ponderada le aplicamos una función de activación delta (no se preocupen veremos a continuación que es esta función). Quedando así finalmente \n",
    "    \n",
    "   $$\\delta(\\sum\\limits_{i=0}^{m} {x_i*w_i})$$ \n",
    "\n",
    "\n",
    "## 2. Función de activación\n",
    "\n",
    "La función de activación es la que me permite definir el comportamiento de mi neurona y de cada capa, esta función decide que valor pasa o se queda. Hablaremos brevemente de 4 de ellas.\n",
    "\n",
    "- Threshold Function o escalón unitario\n",
    "![EstructuraUrl](https://miro.medium.com/max/960/0*etGrZj_m0spvgN9n.png \"estructura\")\n",
    "Esta es la función más simple, como verán cuando recibe valore negativos (menores a 0) la función está en cero y cuando recibe valores positivos (mayores a 0) la función pasa a ser 1, es decir nuestro valor de salida siempre va a ser 1 y 0 nada más. Ahora que comprendemos como es una función de activación básica vamos a uno un poco más compleja.\n",
    "\n",
    "\n",
    "- Función Sigmoide\n",
    "![EstructuraUrl](https://miro.medium.com/max/744/1*zSDNMn4z_hmYing0wDCSoQ.png \"sigmoide\")\n",
    "Como vemos esta función es un poco más suave, pasa por todos los valores entre 0 y 1, es muy últil cuando queremos calcular probabilidades en nuestra salida.\n",
    "\n",
    "\n",
    "- Función Rectificadora\n",
    "![EstructuraUrl](https://miro.medium.com/max/736/1*RthaUGkni6YCtwlAKsadsg.png \"recti\")\n",
    "Es una de las funciones más utilizadas en Machine Learning, esto lo que hace simplemente es entregarnos los valores cuando seán mayores a cero, es decir, valor positivo. Para cada valor positivo un valor de la curva.\n",
    "\n",
    "\n",
    "- Tangente Hiperbólica\n",
    "![EstructuraUrl](https://miro.medium.com/max/772/1*sYnLa9djWZ7BozqUV5lNmw.png \"hiper\")\n",
    "Esta función es muy similar a la Sigmoide pero va desde -1 a 1 los valores.\n",
    "\n",
    "Quisiera que puedas tener en mente esto ya que todavía no entraremos a profundidad pero si los veremos más adelante.\n",
    "\n",
    "## 3. ¿Cómo funcionan las Redes Neuronales Artificiales en un caso real?\n",
    "\n",
    "Vamos a imaginar que hacemos una clasificación, vamos a usar un dataset, en este caso y para ejemplo el de Titanic, donde nos trae una lista de pasajeros y si sobrevivieron o no. A continuación vamos a representar nuestro perceptrón con las variables y se ve así:\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/1084/1*_pD95maLHcEL8Hc39EMJGA.png \"ti\")\n",
    "\n",
    "Para este caso es súper fácil calcular ya que el resultado es simplemente la suma ponderada con la función de activación. Pero que pasa si añado más capacas de a nuestro modelo, es decir, todas las entradas iran conectadas a la capa siguiente como vemos en la siguiente imagen, eso significa que podríamos predecir más valores, aunque no piensen que si tenemos más capa mejora el resultado.\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/1084/1*YjtSTddpDPCscJSXcQblBg.png \"j\")\n",
    "\n",
    "Mientras más capas tengas más nivel de abstracción va a tener nuestra red. Recuerden que básicamente cada una recibe un valor de peso distinto por lo que puede o no recibir esa entrada x en algunos perceptrones, de esta manera, cada uno realiza una función difrente, aunque no siempre sea así.\n",
    "\n",
    "## 4. ¿Cómo aprenden las redes neuronales?\n",
    "\n",
    "En programación hay muchas maneras de llegar a una solución, por ejemplo, podríamos programar todos los casos y respuestas para una solución posible. Cuando esta tarea lleva variables que no siempre se van a comportar como el algoritmo espera, por ejemplo. Queremos crear algo que reconzca un perro en cualquier fotos in importar la raza. Suena sencillo, clasifico las imagenes, digo que son perros, y listo. Pero esta tarea se complica ya que existen n razas de perros, pueden salir en n tipo de ambientes en la fotografía y con diferentes tipos de poses, para esto, es mucho más complicado tener un código que lo resuleva o programar cada caso, para ello se crean las redes neuronales artificiales, claro, para nosotros es natural saber que hay un perro en la foto pero para la máquina no.\n",
    "\n",
    "La ventaja de las redes neuronales es que puede \"entender\" los datos que le damos de entrada y hacer una función para cualquier entrada que le presentemos, y como buen ejemplo hemos dado la imagen del perro.\n",
    "\n",
    "Veamos de nuevo la red neuronal anterior...\n",
    "![EstructuraUrl](https://miro.medium.com/max/1084/1*YjtSTddpDPCscJSXcQblBg.png \"j\")\n",
    "Lo que vamos a hacer es calcular el valor de la salida \"y\" (si murió o no) en base a nuestro dato de entrada \"x\" (variables que vemos arriba). Además de esto le vamos a pasar a Y valores reales de que si murió o no. ¿Pero entonces de que me sirve la predicción en \"y\" si le paso los valores reales?. Es simple, al final de cada paso por la neurona, la máquina lo que va a a hacer es comparar el valor de salida de la predicción con el valor real, esto se hace a través de la función de pérdida y podemos verlo en la siguiente ecuación:\n",
    "$$L = {1 \\over 2}(\\Upsilon - y)^2$$\n",
    "Aquí Upsilon (la y medio rara que se ve ahí jajaja) es nuestro valor predecido y la y normal es nuestro valor real. Esta es la más común entre las funciones de pérdida y esta se llama \"Mean Squered Error\" o la medidad de los errores al cuadrado. Esta función lo que nos dice es cuánto es el error que va a tener nuestra red y lo que tenemos que hacer es minimizarla.\n",
    "Esta función va para atrás y corrije los pesos para ir minizando los pesos \"w\".\n",
    "\n",
    "## 5. ¿Cómo es que minimizo mi función de perdida?\n",
    "\n",
    "Bueno para poder minizar la función de perdida podríamos utilizar algo que se llama el descenso de gradiente (aclaro que este texto no muestra la unica forma pero si trato de simplificar lo máximo posible para adentrarnos en este complejo mundo).\n",
    "Lo que hace esta función es tratar de encontrar los valores de los pesos de modo a que el error se ajuste con la pendiente de la recta dentro del conjunto de valores (puntos azules) que serían las entradas como se aprecia en el siguiente gif:\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/proxy/1*KQVi812_aERFRolz_5G3rA.gif)\n",
    "\n",
    "Pero que pasa si nuestra función no representa una parabola y tiene varios mínimos como se ve a continuación\n",
    "\n",
    "![EstructuraUrl](https://miro.medium.com/max/1400/0*XKMTnu8MwD2wdso6.png)\n",
    "\n",
    "Si nuestro algoritmo solamente encuentra un mínimo local no encontrará un mínimo global que es el que neceitamos para que se ajusten los valores de los pesos.\n",
    "\n",
    "Entonces con el descendo de gradiente lo que nuestra función hace en cada paso es\n",
    "- Paso el dato 1\n",
    "- Paso el dato 2\n",
    "- Paso el dato 3\n",
    "- Ajusto los pesos\n",
    "\n",
    "Para solucionar el problema anterior donde tenemos varios mínimos posibles se utiliza otro algoritmo que hace algo que se llama descenso de gradiente estocástico y hace lo siguiente\n",
    "- Paso el dato 1\n",
    "- Ajusto los pesos\n",
    "- Paso el dato 2\n",
    "- Ajusto los pesos\n",
    "- Paso el dato 3\n",
    "- Ajusto los pesos\n",
    "\n",
    "Entonces cada vez que pase por el dato va a justar los pesos y se acercará más al mínimo global ya que se mueve constantemente ante cada paso de datos.\n",
    "\n",
    "## 5. Backpropagation\n",
    "\n",
    "Todo lo que vimos hasta ahora es lo que se llama comunmente Forward Propagation, donde el valor de entrada, pasa por toda la red, pasa por la función de activación, se calcula la función de pérdida, luego de este proceso la pérdida se debe \"propagar hacia atrás\" ajustando los valores de los pesos, esto es lo que hace el backpropagation, ir para atrás y ajustar los valores de los pesos a la vez en simultáneo, es un algoritmo con unas matemáticas muy avanzadas e interesantes que los puedes mirar por tu cuenta si quieres :)\n",
    "\n",
    "## 6. Manos a la obra\n",
    "\n",
    "Ahora lo que haremos es aplicar una pequeña red neuronal sin utilizar ninguna librería especial excepto la de numpy. Numpy es una libería de python que nos permite realizar operaciones matemáticas complejas con una fácil implementación del código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Como primer paso lo que haremos es instalar esta libería en caso de que nuestra PC no lo tenga, si estás usando algún otro editor puedes instalarlo desde tu terminal quitando el signo de exclamación que tiene en frente.\n",
    "\n",
    "Recuerden que para ejecutar un bloque de código como el que ven abajo solo deben ubicar el cursos sobre el bloque y presionar SHIFT+ENTER y así sucesivamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (1.18.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Importamos la librería de numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lo que hacemos en este punto crear una función que calcule nuestra función de activación, que sería una variante de la función sigmoide.\n",
    "Tengamos en cuenta que creamos esta función para usarlo en sus dos formas, si queremos como sus derivadas o no.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlin(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lo que hacemos aquí abajo es crear dos matrices, una matriz \"x\" que ya sabemos que es la entrada \"y\" que son los valores reales de la salida osea los resultados reales.\n",
    "\n",
    "Recuerden que esos datos que presentamos en la entrada son valores cualquiera, solo representan una entrada que queremos que nos brinde una salida predictiva.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,0,1],\n",
    "      [0,1,1],\n",
    "      [1,0,1],\n",
    "      [1,1,1]])\n",
    "y = np.array([[0],\n",
    "      [1],\n",
    "      [1],\n",
    "      [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Aquí lo que haremos es crear valores de pesos de manera randomica, que la computadora me los genere por que es lo que voy a ir corrigiendo con el algoritmo. Los imprimo para que los veamos.\n",
    "Supongamos que vamos a tener dos capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
      " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n",
      " [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n",
      "[[-0.5910955 ]\n",
      " [ 0.75623487]\n",
      " [-0.94522481]\n",
      " [ 0.34093502]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "w1 = 2*np.random.random((3,4))-1\n",
    "w2 = 2*np.random.random((4,1))-1\n",
    "\n",
    "print(w1)\n",
    "print(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bueno aquí empieza lo divertido 😊\n",
    "Supogamos que L1 y L2 son las capas.\n",
    "Lo que vamos a hacer primero es el #Forward Propagation, en este paso primero guardamos en \"L0\" nuestros valores de \"x\" y luego hacemos un producto punto con los valores de los pesos (suma ponderarda) y lo pasamos por la función de activación como se puestra en L1, estos valores se multiplican con los valores anteriores y los pesos de la capa 2.\n",
    "\n",
    "Luego en #Cálculo de erro lo que hacemos es calcular nuestro error de los valores reales con nuestras predicciones para hallar el valor de pérdida. Esto hacemos restando el valor de real con el valor que se predijo.\n",
    "\n",
    "¿Recuerdan que hablamos sobre el algoritmo de descenso de gradiente?. Bueno eso se calcula multiplicando el error actual por la derivada de la función que creamos con las predicciones actuales (por eso decidimos tener una opción de derivada más arriba) Recuerde la derivada es la recta tangente en un punto a la función.\n",
    "\n",
    "#Backpropagation.. Una vez que tengamos el descenso de gradiente (la pendiente) multimplicamos por la transpuesta de los pesos de la capa siguiente y de este modo propagaremos el error hacia atrás.\n",
    "\n",
    "Y finalmente #ajustamos el valor de los pesos con la transpuesta de las capas por sus respectivas optimizaciones o direcciones hacia donde actualizar los pesos y esto suman a nuestros pesos actuales modificando los dos a la vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(6000):\n",
    "    #Forward Propagation\n",
    "    L0 = x\n",
    "    L1 = nonlin(np.dot(L0,w1))\n",
    "    L2 = nonlin(np.dot(L1,w2)) \n",
    "    \n",
    "    #Cálculo de error\n",
    "    L2_error = y - L2       \n",
    "    \n",
    "    #Calculamos el descenso de gradiente\n",
    "    L2_delta = L2_error*nonlin(L2,deriv=True)\n",
    "    \n",
    "    #Backpropagation\n",
    "    L1_error = L2_delta.dot(w2.T)\n",
    "    L1_delta = L1_error * nonlin(L1,deriv=True)\n",
    "    \n",
    "    #ajustamos el valor de los pesos\n",
    "    w2 += L1.T.dot(L2_delta)\n",
    "    w1 += L0.T.dot(L1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora analicemos los resultados, vemos como aquí nuestro valores \"y\" que son los reales vs los valores que la máquina predijo que serían. ¿Muy loco no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores reales\n",
      "\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "\n",
      "Valores de las predicciones\n",
      "\n",
      "[[0.00631758]\n",
      " [0.99192018]\n",
      " [0.99291766]\n",
      " [0.0094266 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Valores reales\\n\")\n",
    "print(y)\n",
    "print(\"\\n\")\n",
    "print(\"Valores de las predicciones\\n\")\n",
    "print(L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
